{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41511502-8330-45ea-a1d1-70fed751785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are missing from the target environment:\n",
      "  - autogluon\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda uninstall autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809943c6-9e73-4db0-b488-1712332f816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are missing from the target environment:\n",
      "  - autogluon.tabular\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda uninstall autogluon.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b632846-d641-46c7-8e86-3997f3954850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogluon\n",
      "  Using cached autogluon-1.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: autogluon.core==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core[all]==1.0.0->autogluon) (1.0.0)\n",
      "Requirement already satisfied: autogluon.features==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon) (1.0.0)\n",
      "Requirement already satisfied: autogluon.tabular==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular[all]==1.0.0->autogluon) (1.0.0)\n",
      "Collecting autogluon.multimodal==1.0.0 (from autogluon)\n",
      "  Using cached autogluon.multimodal-1.0.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting autogluon.timeseries==1.0.0 (from autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached autogluon.timeseries-1.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<1.29,>=1.21 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.13,>=1.5.4 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn<1.5,>=1.3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.4.1.post1)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (3.1)\n",
      "Requirement already satisfied: pandas<2.2.0,>=2.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (2.1.4)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (2.31.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (3.8.0)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.34.54)\n",
      "Requirement already satisfied: autogluon.common==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.0.0)\n",
      "Collecting ray<2.7,>=2.6.3 (from ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Downloading ray-2.6.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting async-timeout (from autogluon.core[all]==1.0.0->autogluon)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting hyperopt<0.2.8,>=0.2.7 (from autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: Pillow<11,>=10.0.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.0.0->autogluon) (10.2.0)\n",
      "Collecting torch<2.1,>=2.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading torch-2.0.1-cp311-cp311-win_amd64.whl.metadata (24 kB)\n",
      "Collecting lightning<2.1,>=2.0.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached lightning-2.0.9.post0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting jsonschema<4.18,>=4.14 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting transformers<4.32.0,>=4.31.0 (from transformers[sentencepiece]<4.32.0,>=4.31.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Collecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torchvision<0.16.0,>=0.14.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading torchvision-0.15.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scikit-image<0.21.0,>=0.19.1 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading scikit_image-0.20.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.0.0->autogluon) (1.3)\n",
      "Collecting torchmetrics<1.2.0,>=1.0.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached torchmetrics-1.1.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytorch-metric-learning<2.0,>=1.3.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached pytorch_metric_learning-1.7.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.0.0->autogluon) (3.8.1)\n",
      "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.0.0->autogluon) (0.7.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=3.0.3 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.0.0->autogluon) (3.1.3)\n",
      "Collecting tensorboard<3,>=2.9 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting xgboost<2.1,>=1.6 (from autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting fastai<2.8,>=2.3.1 (from autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached fastai-2.7.14-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting lightgbm<4.2,>=3.3 (from autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached lightgbm-4.1.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Collecting catboost<1.3,>=1.1 (from autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Downloading catboost-1.2.3-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon) (1.2.0)\n",
      "Collecting pytorch-lightning<2.1,>=2.0.0 (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached pytorch_lightning-2.0.9.post0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: statsmodels<0.15,>=0.13.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon) (0.14.0)\n",
      "Collecting gluonts<0.15,>=0.14.0 (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached gluonts-0.14.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached statsforecast-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached mlforecast-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached utilsforecast-0.0.10-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting orjson~=3.9 (from autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Downloading orjson-3.9.15-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.7/50.7 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.common==1.0.0->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (5.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.common==1.0.0->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.0.0->autogluon) (23.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.0.0->autogluon) (6.0.1)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.54 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.34.54)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (0.10.0)\n",
      "Collecting graphviz (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: plotly in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.0.0->autogluon) (5.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.0.0->autogluon) (1.16.0)\n",
      "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: dill in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon) (0.3.7)\n",
      "Collecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon) (2023.10.0)\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting responses<0.19 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.0.0->autogluon) (23.3.1)\n",
      "Collecting fastdownload<2,>=0.0.5 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached fastdownload-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting fastcore<1.6,>=1.5.29 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached fastcore-1.5.29-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting fastprogress>=0.2.4 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Using cached fastprogress-1.0.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting spacy<4 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.0.0->autogluon)\n",
      "  Downloading spacy-3.7.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from gluonts<0.15,>=0.14.0->autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon) (1.10.12)\n",
      "Requirement already satisfied: toolz~=0.10 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from gluonts<0.15,>=0.14.0->autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from gluonts<0.15,>=0.14.0->autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon) (4.9.0)\n",
      "Requirement already satisfied: future in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.0.0->autogluon) (0.18.3)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.0.0->autogluon) (2.2.1)\n",
      "Collecting py4j (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.0.0->autogluon) (2.1.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==1.0.0->autogluon) (23.1.0)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema<4.18,>=4.14->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading pyrsistent-0.20.0-cp311-cp311-win_amd64.whl.metadata (976 bytes)\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (1.2.3)\n",
      "Collecting backoff<4.0,>=2.2.1 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (4.12.2)\n",
      "Requirement already satisfied: click<10.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (8.1.7)\n",
      "Collecting croniter<1.5.0,>=1.3.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached croniter-1.4.1-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Collecting dateutils<2.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached dateutils-0.6.12-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting deepdiff<8.0,>=5.7.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached deepdiff-6.7.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting fastapi<2.0,>=0.92.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached fastapi-0.110.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting inquirer<5.0,>=2.10.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached inquirer-3.2.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting lightning-cloud>=0.5.38 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached lightning_cloud-0.5.64-py3-none-any.whl.metadata (933 bytes)\n",
      "Collecting lightning-utilities<2.0,>=0.7.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting python-multipart<2.0,>=0.0.5 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (13.3.5)\n",
      "Collecting starlette (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached starlette-0.37.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting starsessions<2.0,>=1.2.1 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached starsessions-1.3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (5.7.1)\n",
      "Requirement already satisfied: urllib3<4.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (2.0.7)\n",
      "Collecting uvicorn<2.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached uvicorn-0.27.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: websocket-client<3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon) (0.58.0)\n",
      "Collecting websockets<13.0 (from lightning<2.1,>=2.0.0->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Downloading websockets-12.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numba in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon) (0.59.0)\n",
      "Collecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.0.0->autogluon.timeseries[all]==1.0.0->autogluon)\n",
      "  Using cached window_ops-0.0.14-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.0.0->autogluon) (2023.10.3)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.0.0->autogluon) (0.4.6)\n",
      "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.0.0->autogluon)\n",
      "  Using cached opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.0.0->autogluon) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=2.0.0->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=2.0.0->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=2.0.0->autogluon.core==1.0.0->autogluon.core[all]==1.0.0->autogluon) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray<2.7,>=2.6.3->ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (3.13.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray<2.7,>=2.6.3->ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (1.0.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray<2.7,>=2.6.3->ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (3.20.3)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray<2.7,>=2.6.3->ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (1.2.0)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray<2.7,>=2.6.3->ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (1.4.0)\n",
      "Collecting grpcio>=1.42.0 (from ray<2.7,>=2.6.3->ray[default]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Downloading grpcio-1.62.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: aiohttp>=3.7 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (3.9.3)\n",
      "Collecting aiohttp-cors (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting colorful (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting py-spy>=0.2.0 (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached py_spy-0.3.14-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting gpustat>=1.0.0 (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached gpustat-1.1.1.tar.gz (98 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting opencensus (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (0.14.1)\n",
      "Requirement already satisfied: smart-open in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon) (5.2.1)\n",
      "Collecting virtualenv<20.21.1,>=20.0.24 (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached virtualenv-20.21.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pyarrow<7.0.0,>=6.0.1 (from ray[default,tune]<2.7,>=2.6.3; extra == \"all\"->autogluon.core[all]==1.0.0->autogluon)\n",
      "  Downloading pyarrow-6.0.1.tar.gz (770 kB)\n",
      "     ---------------------------------------- 0.0/770.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 770.7/770.7 kB 24.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [8 lines of output]\n",
      "  Ignoring numpy: markers 'python_version < \"3.8\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.8\"' don't match your environment\n",
      "  Ignoring numpy: markers 'python_version == \"3.9\"' don't match your environment\n",
      "  Collecting cython>=0.29\n",
      "    Downloading Cython-3.0.8-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "  ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "  ERROR: Could not find a version that satisfies the requirement numpy==1.21.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0rc1, 1.23.0rc2, 1.23.0rc3, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0rc1, 1.24.0rc2, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0rc1, 1.25.0, 1.25.1, 1.25.2, 1.26.0b1, 1.26.0rc1, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4)\n",
      "  ERROR: No matching distribution found for numpy==1.21.3\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e802f5a2-96d5-4780-bb1a-2e8715567cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogluon.tabular in c:\\users\\untjr\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy<1.29,>=1.21 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.13,>=1.5.4 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (1.11.4)\n",
      "Requirement already satisfied: pandas<2.2.0,>=2.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn<1.5,>=1.3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (1.4.1.post1)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (3.1)\n",
      "Requirement already satisfied: autogluon.core==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (1.0.0)\n",
      "Requirement already satisfied: autogluon.features==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.tabular) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.tabular) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.tabular) (2.31.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.tabular) (3.8.0)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.tabular) (1.34.54)\n",
      "Requirement already satisfied: autogluon.common==1.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.core==1.0.0->autogluon.tabular) (1.0.0)\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.common==1.0.0->autogluon.core==1.0.0->autogluon.tabular) (5.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from autogluon.common==1.0.0->autogluon.core==1.0.0->autogluon.tabular) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=2.0.0->autogluon.tabular) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=2.0.0->autogluon.tabular) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from pandas<2.2.0,>=2.0.0->autogluon.tabular) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from scikit-learn<1.5,>=1.3.0->autogluon.tabular) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from scikit-learn<1.5,>=1.3.0->autogluon.tabular) (2.2.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.54 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.0.0->autogluon.tabular) (1.34.54)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.0.0->autogluon.tabular) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.0.0->autogluon.tabular) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.2.0,>=2.0.0->autogluon.tabular) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.38->autogluon.core==1.0.0->autogluon.tabular) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from matplotlib->autogluon.core==1.0.0->autogluon.tabular) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.0.0->autogluon.tabular) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.0.0->autogluon.tabular) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.0.0->autogluon.tabular) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\untjr\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.0.0->autogluon.tabular) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install autogluon.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f55ad46-3077-4db5-8251-16e281bcb3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrive/MyDrive/Colab\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m Notebooks/dacon/dacon-9\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd drive/MyDrive/Colab\\ Notebooks/dacon/dacon-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80477af6-0fd6-4a4d-8ab0-abf62e5c0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 저희는 Tabular Data를 다루기 때문에 아래 라이브러리를 호출합니다.\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9357515-138b-40d9-8c48-94b1dcf6e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36489f8-3a7a-432c-adae-c3b4d8ab80cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>요일</th>\n",
       "      <th>본사정원수</th>\n",
       "      <th>본사휴가자수</th>\n",
       "      <th>본사출장자수</th>\n",
       "      <th>본사시간외근무명령서승인건수</th>\n",
       "      <th>현본사소속재택근무자수</th>\n",
       "      <th>조식메뉴</th>\n",
       "      <th>중식메뉴</th>\n",
       "      <th>석식메뉴</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>수</td>\n",
       "      <td>2983</td>\n",
       "      <td>88</td>\n",
       "      <td>182</td>\n",
       "      <td>5</td>\n",
       "      <td>358.0</td>\n",
       "      <td>모닝롤/연유버터베이글 우유/주스 계란후라이/찐계란 단호박죽/흑미밥 우거지국 고기완자...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 대구지리 매운돈갈비찜 오꼬노미계란말이 상추무침 포기김치 양상추...</td>\n",
       "      <td>흑미밥 얼큰순두부찌개 쇠고기우엉볶음 버섯햄볶음 (New)아삭이고추무절임 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>목</td>\n",
       "      <td>2983</td>\n",
       "      <td>104</td>\n",
       "      <td>212</td>\n",
       "      <td>409</td>\n",
       "      <td>348.0</td>\n",
       "      <td>모닝롤/대만샌드위치 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 황태국 시래기지짐 ...</td>\n",
       "      <td>쌀밥/보리밥/찰현미밥 우렁된장찌개 오리주물럭 청양부추전 수제삼색무쌈 겉절이김치 양상...</td>\n",
       "      <td>충무김밥 우동국물 오징어무침 꽃맛살샐러드 얼갈이쌈장무침 석박지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>금</td>\n",
       "      <td>2983</td>\n",
       "      <td>270</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>모닝롤/핫케익 우유/주스 계란후라이/찐계란 오곡죽/흑미밥 매생이굴국 고구마순볶음 양...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 팽이장국 수제돈까스*소스 가자미조림 동초나물무침 포기김치 양상...</td>\n",
       "      <td>흑미밥 물만둣국 카레찜닭 숯불양념꼬지어묵 꼬시래기무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>월</td>\n",
       "      <td>2924</td>\n",
       "      <td>108</td>\n",
       "      <td>154</td>\n",
       "      <td>538</td>\n",
       "      <td>322.0</td>\n",
       "      <td>모닝롤/촉촉한치즈케익 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 두부김칫국 새우완...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 배추들깨국 오리대패불고기 시금치프리타타 부추고추장무침 포기김치...</td>\n",
       "      <td>흑미밥 동태탕 돈육꽈리고추장조림 당면채소무침 모자반무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>화</td>\n",
       "      <td>2924</td>\n",
       "      <td>62</td>\n",
       "      <td>186</td>\n",
       "      <td>455</td>\n",
       "      <td>314.0</td>\n",
       "      <td>모닝롤/토마토샌드 우유/주스 계란후라이/찐계란 채소죽/흑미밥 호박맑은국 오이생채 양...</td>\n",
       "      <td>쌀밥/팥밥/찰현미밥 부대찌개 닭살데리야끼조림 버섯탕수 세발나물무침 알타리김치/사과푸...</td>\n",
       "      <td>흑미밥 바지락살국 쇠고기청경채볶음 두부구이*볶은김치 머위된장무침 백김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>수</td>\n",
       "      <td>2924</td>\n",
       "      <td>59</td>\n",
       "      <td>199</td>\n",
       "      <td>5</td>\n",
       "      <td>286.0</td>\n",
       "      <td>모닝롤/게살모닝샌드 우유/주스 계란후라이/찐계란 소고기죽/흑미밥 시래기된장국 베이컨...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 아욱국 매콤해물볶음 감자조림 미나리나물 포기김치 콥샐러드*렌치D</td>\n",
       "      <td>오므라이스 가쓰오장국 빌소세지구이*구운채소 단감치커리무침 양념고추지 겉절이김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-02-04</td>\n",
       "      <td>목</td>\n",
       "      <td>2924</td>\n",
       "      <td>61</td>\n",
       "      <td>211</td>\n",
       "      <td>476</td>\n",
       "      <td>288.0</td>\n",
       "      <td>모닝롤/사과파이 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 아욱국 새송이버섯곤약장...</td>\n",
       "      <td>쌀밥/차조밥/찰현미밥 설렁탕 고등어김치말이찜 볼어묵굴소스볶음 브로콜리숙회*초장 석박...</td>\n",
       "      <td>흑미밥 계란파국 돈육두루치기 감자채파프리카볶음 세발나물오리엔탈무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-05</td>\n",
       "      <td>금</td>\n",
       "      <td>2924</td>\n",
       "      <td>169</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>모닝롤/앙버터모닝빵 우유/주스 계란후라이/찐계란 고구마죽/흑미밥 옹심이국 머위나물무...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 북엇국 닭볶음탕 채소전*장 솎음열무나물무침 포기김치 양상추샐러...</td>\n",
       "      <td>유부초밥/추가밥 온메밀소바 국물떡볶이 순대찜*소금 청경채겉절이 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>월</td>\n",
       "      <td>2924</td>\n",
       "      <td>88</td>\n",
       "      <td>174</td>\n",
       "      <td>690</td>\n",
       "      <td>329.0</td>\n",
       "      <td>모닝롤/스콘 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 꽃게탕 근대나물무침 연두부...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 감자양파국 돈수육*씨앗쌈장 매콤어묵볶음 콩나물파채무침 포기김치...</td>\n",
       "      <td>흑미밥 냉이국 반반치킨 꼬막채소무침 청경채찜 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>화</td>\n",
       "      <td>2924</td>\n",
       "      <td>94</td>\n",
       "      <td>183</td>\n",
       "      <td>542</td>\n",
       "      <td>329.0</td>\n",
       "      <td>모닝롤/치즈팡샌드 우유/주스 계란후라이/찐계란 팥죽/흑미밥 맑은버섯국 시금치나물무침...</td>\n",
       "      <td>쌀밥/기장밥/찰현미밥 장각백숙 적어양념장구이 채소스틱*쌈장 도라지오이초무침 겉절이김...</td>\n",
       "      <td>흑미밥 미역국 매운소불고기 단호박두부탕수 메추리알장조림 석박지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>수</td>\n",
       "      <td>2924</td>\n",
       "      <td>489</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>모닝롤/길거리토스트 우유/주스 계란후라이/찐계란 옥수수스프/흑미밥 우거지국 스팸구이...</td>\n",
       "      <td>유니짜장밥 짬뽕국 수제찹쌀꿔바로우 계란후라이 단무지락교무침 포기김치 그린샐러드*딸기...</td>\n",
       "      <td>흑미밥 참치김치찌개 오징어굴소스볶음 차돌비빔국수 건새우무나물 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-02-15</td>\n",
       "      <td>월</td>\n",
       "      <td>2924</td>\n",
       "      <td>178</td>\n",
       "      <td>131</td>\n",
       "      <td>795</td>\n",
       "      <td>355.0</td>\n",
       "      <td>모닝롤/파운드케익 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 열무된장국 분홍소세지...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 떡국 소갈비찜 한식잡채 참나물겉절이 포기김치 양상추샐러드*블루...</td>\n",
       "      <td>흑미밥 순두부백탕 수제치킨까스 쫄면채소무침 얼갈이나물 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-02-16</td>\n",
       "      <td>화</td>\n",
       "      <td>2924</td>\n",
       "      <td>70</td>\n",
       "      <td>175</td>\n",
       "      <td>815</td>\n",
       "      <td>413.0</td>\n",
       "      <td>모닝롤/모닝샌드 우유/주스 계란후라이/찐계란 흑임자죽/흑미밥 대구매운탕 가지나물 양...</td>\n",
       "      <td>쌀밥/수수밥/찰현미밥 육개장 닭살겨자냉채 오이스틱*쌈장 탕평채 깍두기/수박 양상추샐...</td>\n",
       "      <td>흑미밥 손수제비국 쇠고기낙지볶음 카레홍합찜 쑥갓나물 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>수</td>\n",
       "      <td>2924</td>\n",
       "      <td>77</td>\n",
       "      <td>181</td>\n",
       "      <td>3</td>\n",
       "      <td>379.0</td>\n",
       "      <td>모닝롤/트위스터버거 우유/주스 계란후라이/찐계란 단호박크림스프/흑미밥 사골파국 양념...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 미니쌀국수 삼겹살고추장구이 스프링롤*타르타르D 동초나물무침 알...</td>\n",
       "      <td>곤드레밥 황태국 찰떡떡갈비조림 계란후라이 재래김*달래양념장 무생채</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>목</td>\n",
       "      <td>2924</td>\n",
       "      <td>83</td>\n",
       "      <td>247</td>\n",
       "      <td>594</td>\n",
       "      <td>338.0</td>\n",
       "      <td>모닝롤/허니브레드 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 봄동된장국 참치채소볶...</td>\n",
       "      <td>쌀밥/완두콩밥/찰현미밥 김치어묵탕 수원왕갈비통닭 두부양념조림 연근깨소스무침 포기김치...</td>\n",
       "      <td>흑미밥 바지락된장찌개 제육볶음 양배추숙*쌈장 노가리고추조림 겉절이김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>금</td>\n",
       "      <td>2924</td>\n",
       "      <td>176</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>280.0</td>\n",
       "      <td>모닝롤/크로와상샌드위치 우유/주스 계란후라이/찐계란 쇠고기죽/흑미밥 닭살해장국 해물...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 유부장국 해물누룽지탕 김치전 마약계란장조림 포기김치 양상추샐러...</td>\n",
       "      <td>흑미밥 버섯들깨탕 아귀콩나물찜 콤비네이션피자 돌나물&amp;된장소스 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-02-22</td>\n",
       "      <td>월</td>\n",
       "      <td>2924</td>\n",
       "      <td>105</td>\n",
       "      <td>197</td>\n",
       "      <td>814</td>\n",
       "      <td>247.0</td>\n",
       "      <td>모닝롤/흑미쌀찐빵 우유/주스 계란후라이 누룽지탕/흑미밥 미역국 매콤부들어묵볶음 양상...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 호박고추장찌개 안동찜닭 마카로니치즈범벅 세발나물무침 포기김치/...</td>\n",
       "      <td>흑미밥 동태알탕 깐풍육 고사리볶음 오이무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>화</td>\n",
       "      <td>2924</td>\n",
       "      <td>75</td>\n",
       "      <td>200</td>\n",
       "      <td>783</td>\n",
       "      <td>233.0</td>\n",
       "      <td>모닝롤/프렌치토스트 우유/주스 계란후라이 녹두죽/흑미밥 북어해장국 부추김무침 양상추...</td>\n",
       "      <td>쌀밥/보리밥/찰현미밥 근대국 등갈비김치찜 감자채전*장 치커리무침 깍두기 파스타샐러드</td>\n",
       "      <td>흑미밥 쇠고기무국 춘전닭갈비 뉴욕핫도그 유채나물된장무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>수</td>\n",
       "      <td>2924</td>\n",
       "      <td>77</td>\n",
       "      <td>235</td>\n",
       "      <td>3</td>\n",
       "      <td>234.0</td>\n",
       "      <td>모닝롤/파게트 우유/주스 계란후라이 감자스프/흑미밥 시금치된장국 우엉조림 양상추샐러...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 해물탕 쇠고기숙주볶음 맛살계란말이 물미역초고추장무침 포기김치 ...</td>\n",
       "      <td>애플카레라이스 팽이장국 가지탕수 소떡소떡 오복지무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>목</td>\n",
       "      <td>2924</td>\n",
       "      <td>91</td>\n",
       "      <td>252</td>\n",
       "      <td>585</td>\n",
       "      <td>235.0</td>\n",
       "      <td>모닝롤/야채모닝샌드 우유/주스 계란후라이 누룽지탕/흑미밥 맑은버섯국 수제동그랑땡전*...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 나주곰탕 생선까스*타르타르D 더덕양념구이 방풍나물무침 석박지 ...</td>\n",
       "      <td>흑미밥 계란파국 쭈꾸미불고기 모둠채소전*장 씨앗콩자반 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>금</td>\n",
       "      <td>2924</td>\n",
       "      <td>261</td>\n",
       "      <td>279</td>\n",
       "      <td>1</td>\n",
       "      <td>179.0</td>\n",
       "      <td>모닝롤/베이컨맥모닝 우유/주스 계란후라이 게살죽/흑미밥 매생이굴국 두부양념구이 양상...</td>\n",
       "      <td>쌀밥/오곡밥/찰현미밥 옹심이국 목살스테이크 베이비크랩강정 이색나물(호박고지,건취나물...</td>\n",
       "      <td>흑미밥 스팸김치찌개 삼치구이*와사비장 브로콜리깨소스무침 연근조림 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>화</td>\n",
       "      <td>2975</td>\n",
       "      <td>139</td>\n",
       "      <td>166</td>\n",
       "      <td>781</td>\n",
       "      <td>248.0</td>\n",
       "      <td>모닝롤/호떡 우유/주스 계란후라이 누룽지탕/흑미밥 애호박새우젓국 재래김*양념장 양상...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 아욱국 치즈불닭 베이컨감자볶음 매운콩나물무침 포기김치 양배추샐...</td>\n",
       "      <td>흑미밥 냉이김칫국 해물우동볶음 날치알계란찜 솎음열무나물 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-03-03</td>\n",
       "      <td>수</td>\n",
       "      <td>2975</td>\n",
       "      <td>50</td>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "      <td>292.0</td>\n",
       "      <td>모닝롤/크림치즈와플 우유/주스 계란후라이 고구마죽/흑미밥 사골우거지국 비엔나소세지볶...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 황태미역국 동파육 느타리버섯볶음 참나물상추겉절이 포기김치/망고...</td>\n",
       "      <td>흑미밥 (New)수제오떡탕 매운족발볶음 크래미오이보트샐러드 청경채나물 겉절이김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>목</td>\n",
       "      <td>2975</td>\n",
       "      <td>72</td>\n",
       "      <td>236</td>\n",
       "      <td>746</td>\n",
       "      <td>263.0</td>\n",
       "      <td>모닝롤/BLT샌드 우유/주스 계란후라이 누룽지탕/흑미밥 아귀매운탕 동초나물무침 양상...</td>\n",
       "      <td>쌀밥/옥수수밥/찰현미밥 매운쇠고기샤브샤브국 갈치조림 수수부꾸미 쑥갓두부무침 알타리김...</td>\n",
       "      <td>흑미밥 짬뽕국 쇠고기탕수 고추잡채*꽃빵 해초배무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>금</td>\n",
       "      <td>2975</td>\n",
       "      <td>158</td>\n",
       "      <td>257</td>\n",
       "      <td>2</td>\n",
       "      <td>226.0</td>\n",
       "      <td>모닝롤/모닝사라다빵 우유/주스 계란후라이 낙지죽/흑미밥 감자국 멸치마늘종볶음 양상추...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 쑥국 닭다리튀김 골뱅이채소무침 미나리나물 포기김치 양상추샐러드...</td>\n",
       "      <td>샐러드김밥 미소시루 라볶이 상추튀김(모둠튀김*양념장) 단무지채무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-03-08</td>\n",
       "      <td>월</td>\n",
       "      <td>2975</td>\n",
       "      <td>97</td>\n",
       "      <td>170</td>\n",
       "      <td>939</td>\n",
       "      <td>264.0</td>\n",
       "      <td>모닝롤/프레즐 우유/주스 계란후라이 누룽지탕/흑미밥 맑은버섯국 스팸구이 양상추샐러드...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 얼갈이된장국 오리불고기 (New)순대탕수 깻잎무쌈 포기김치 양...</td>\n",
       "      <td>흑미밥 달래된장찌개 코코뱅 고구마치즈구이 치커리무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-03-09</td>\n",
       "      <td>화</td>\n",
       "      <td>2975</td>\n",
       "      <td>76</td>\n",
       "      <td>170</td>\n",
       "      <td>1003</td>\n",
       "      <td>282.0</td>\n",
       "      <td>모닝롤/계란빵 우유/주스 계란후라이 채소죽/흑미밥 동태매운탕 모둠사태조림 양상추샐러...</td>\n",
       "      <td>쌀밥/차조밥/찰현미밥 갈비탕 순살닭강정 매생이전 도라지오이생채 깍두기 양상추샐러드*...</td>\n",
       "      <td>흑미밥 맑은콩나물국 돈육김치볶음 수제두부동그랑땡 유채나물무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-03-10</td>\n",
       "      <td>수</td>\n",
       "      <td>2975</td>\n",
       "      <td>71</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>271.0</td>\n",
       "      <td>모닝롤/모닝샌드 우유/주스 계란후라이 팥죽/흑미밥 북엇국 방풍나물 양상추샐러드/사과...</td>\n",
       "      <td>봄나물비빔밥 냉이된장국 수제고기육전 도토리묵*양념장 쥬시쿨 포기김치 콥샐러드*렌치D</td>\n",
       "      <td>흑미밥 순두부백탕 낙지볶음 쇠고기들깨소스무침 쪽파무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>목</td>\n",
       "      <td>2975</td>\n",
       "      <td>93</td>\n",
       "      <td>223</td>\n",
       "      <td>609</td>\n",
       "      <td>261.0</td>\n",
       "      <td>모닝롤/(New)당근크림치즈베이글 우유/주스 계란후라이 누룽지탕/흑미밥 조랭이떡국 ...</td>\n",
       "      <td>쌀밥/귀리밥/찰현미밥 콩가루배추국 타워함박스테이크 문어꽈리고추조림 시금치고추장나물무...</td>\n",
       "      <td>꽁보리밥*볶음고추장 닭칼국수 왕만두찜*양념장 버섯맛살볶음 양파장아찌 얼갈이열무겉절이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-03-12</td>\n",
       "      <td>금</td>\n",
       "      <td>2975</td>\n",
       "      <td>241</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>모닝롤/토마토샌드 우유/주스 계란후라이 참치죽/흑미밥 꽃게탕 섭산적채소조림 양상추샐...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 어묵매운탕 목살구이 쌈채소*쌈장 부추무침 겉절이김치 양상추샐러...</td>\n",
       "      <td>흑미밥 뼈해장국 가자미유린기 매운감자조림 배추흑임자무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>월</td>\n",
       "      <td>2975</td>\n",
       "      <td>126</td>\n",
       "      <td>148</td>\n",
       "      <td>864</td>\n",
       "      <td>314.0</td>\n",
       "      <td>모닝롤/고구마파이 우유/주스 계란후라이 누룽지탕/흑미밥 미역국 새우완자전*케찹 연두...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 근대된장국 묵은지닭찜 비엔나브로콜리볶음 유부채소겨자냉채 깍두기...</td>\n",
       "      <td>해물짜장면 계란파국 사천탕수육 세발나물무침 짜사이볶음 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>화</td>\n",
       "      <td>2975</td>\n",
       "      <td>74</td>\n",
       "      <td>176</td>\n",
       "      <td>658</td>\n",
       "      <td>401.0</td>\n",
       "      <td>모닝롤/미니햄버거 우유/주스 계란후라이 오곡죽/흑미밥 대구지리 생깻잎지 양상추샐러드...</td>\n",
       "      <td>쌀밥/수수밥/찰현미밥 대파육개장 고등어구이*와사비장 어묵잡채 건다래순볶음 포기김치 ...</td>\n",
       "      <td>흑미밥 사골파국*소면사리 매콤돈육메추리알장조림 명엽채볶음 참나물초장무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2021-03-17</td>\n",
       "      <td>수</td>\n",
       "      <td>2975</td>\n",
       "      <td>90</td>\n",
       "      <td>192</td>\n",
       "      <td>4</td>\n",
       "      <td>371.0</td>\n",
       "      <td>모닝롤/치즈볼 우유/주스 계란후라이 양송이스프/흑미밥 매운감자양파국 자반무침 양상추...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 물만둣국 제육미나리볶음 두부까스*소스 아삭이고추된장무침 겉절이...</td>\n",
       "      <td>흑미밥 김칫국 닭살채소굴소스볶음 애호박나물 알배기,케일숙쌈*쌈장 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>목</td>\n",
       "      <td>2975</td>\n",
       "      <td>106</td>\n",
       "      <td>231</td>\n",
       "      <td>520</td>\n",
       "      <td>351.0</td>\n",
       "      <td>모닝롤/(New)밤크림빵 우유/주스 계란후라이 누룽지탕/흑미밥 콩나물오징어국 고사리...</td>\n",
       "      <td>쌀밥/기장밥/찰현미밥 열무된장국 장각허브오븐구이*청양마요소스 수제오미산적 머위나물 ...</td>\n",
       "      <td>쇠고기규동덮밥 가쓰오장국 (New)피자핫도그 연근땅콩조림 청경채겉절이 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>금</td>\n",
       "      <td>2975</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>모닝롤/인기가요샌드 우유/주스 계란후라이 흑임자죽/흑미밥 건새우아욱국 베이컨숙주볶음...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 버섯매운탕 돈갈비찜 (New)단호박계란찜 오이생채 포기김치 그...</td>\n",
       "      <td>흑미밥 부대찌개 삼치엿장구이 부추전 치커리사과무침 무생채</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>월</td>\n",
       "      <td>2975</td>\n",
       "      <td>133</td>\n",
       "      <td>166</td>\n",
       "      <td>707</td>\n",
       "      <td>350.0</td>\n",
       "      <td>모닝롤/치즈케익 우유/주스 계란후라이 누룽지탕/흑미밥 시금치된장국 캔꽁치무조림 양상...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 황태국 콩나물불고기 쇠고기납작당면볶음 삼색유자청무침 포기김치 ...</td>\n",
       "      <td>흑미밥 순두부찌개 닭간장조림 매콤어묵볶음 미나리숙주나물 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021-03-23</td>\n",
       "      <td>화</td>\n",
       "      <td>2975</td>\n",
       "      <td>82</td>\n",
       "      <td>205</td>\n",
       "      <td>688</td>\n",
       "      <td>349.0</td>\n",
       "      <td>모닝롤/카야잼샌드 우유/주스 계란후라이 게살죽/흑미밥 들깨무채국 명엽채고추장볶음 양...</td>\n",
       "      <td>쌀밥/검정콩밥/찰현미밥 순남시래기국 장어강정*데리야끼소스 깻잎쌈*생강채 유채나물된장...</td>\n",
       "      <td>베이컨김치볶음밥 우동국물 알리오올리오 계란후라이 수제오이피클 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>수</td>\n",
       "      <td>2975</td>\n",
       "      <td>87</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>모닝롤/크로와상샌드 우유/주스 계란후라이 흑미두부죽/흑미밥 냉이된장국 쇠고기꽈리고추...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 미역국 바베큐폭립 건새우호박채전 비름나물 포기김치 양상추샐러드...</td>\n",
       "      <td>흑미밥 맑은버섯국 오삼불고기 양상추, 쇠미역쌈*강된장 고들빼기무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>목</td>\n",
       "      <td>2975</td>\n",
       "      <td>118</td>\n",
       "      <td>260</td>\n",
       "      <td>441</td>\n",
       "      <td>297.0</td>\n",
       "      <td>모닝롤/치아바타샌드 우유/주스 계란후라이 누룽지탕/흑미밥 쇠고기해장국 감자채볶음 양...</td>\n",
       "      <td>쌀밥/차조밥/찰현미밥 아욱국 짜파치킨 쫄면채소무침 취나물무침 포기김치 양배추샐러드*...</td>\n",
       "      <td>흑미밥 짬뽕수제비 생선까스*타르타르D 더덕무침 쑥갓무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>금</td>\n",
       "      <td>2975</td>\n",
       "      <td>311</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>모닝롤/베이컨에그슬럿 우유/주스 계란후라이 단호박죽/흑미밥 아귀매운탕 땅콩조림 양상...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 돈육김치찌개 소불고기 가지나물 풋마늘대무침 깍두기 양상추샐러드...</td>\n",
       "      <td>흑미밥/미니팥칼국수 차돌된장찌개 적어양념구이 미트볼채소볶음 오복지무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>월</td>\n",
       "      <td>2975</td>\n",
       "      <td>121</td>\n",
       "      <td>178</td>\n",
       "      <td>660</td>\n",
       "      <td>318.0</td>\n",
       "      <td>모닝롤/깨찰빵 우유/주스 계란후라이 누룽지탕/흑미밥 우거지국 마늘종호두조림 양상추샐...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 동태매운탕 차돌박이구이&amp;청경채찜 메추리알떡볶이 세발나물무침 포...</td>\n",
       "      <td>셀프충무김밥 미소시루 오징어어묵무침 콩나물간장볶음 꽃맛살샐러드 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>화</td>\n",
       "      <td>2975</td>\n",
       "      <td>83</td>\n",
       "      <td>198</td>\n",
       "      <td>625</td>\n",
       "      <td>313.0</td>\n",
       "      <td>모닝롤/대만샌드위치 우유/주스 계란후라이 채소새우죽/흑미밥 옹심이국 멸치고추장볶음 ...</td>\n",
       "      <td>쌀밥/완두콩밥/찰현미밥 유부장국 돈수육 브로콜리땅콩소스무침 모듬채소*쌈장 수제보쌈김...</td>\n",
       "      <td>오므라이스/추가밥 애호박새우젓국 빌소세지구이*구운채소 도토리묵채소무침 수제연근유자피...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>수</td>\n",
       "      <td>2975</td>\n",
       "      <td>86</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>모닝롤/모닝샌드 우유/주스 계란후라이 크루통크림스프/흑미밥 꽃게탕 건파래볶음 양상추...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 콩가루배추국 허니순살치킨 버섯초장무침 방풍나물 포기김치 양상추...</td>\n",
       "      <td>흑미밥 쇠고기무국 마파두부소스 납작군만두*장 참나물생채무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>목</td>\n",
       "      <td>2973</td>\n",
       "      <td>88</td>\n",
       "      <td>256</td>\n",
       "      <td>394</td>\n",
       "      <td>303.0</td>\n",
       "      <td>모닝롤/단호박피자빵 우유/주스 계란후라이 누룽지탕/흑미밥 북어해장국 달래오이무침 양...</td>\n",
       "      <td>쌀밥/보리밥/찰현미밥 순대국밥*다대기 해물동그랑땡채소볶음 통들깨부추무침 채소스틱 석...</td>\n",
       "      <td>흑미밥 미니우동 치킨까스김치나베 미나리전 양념고추지 열무김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>금</td>\n",
       "      <td>2973</td>\n",
       "      <td>275</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>모닝롤/야채샌드위치 우유/주스 계란후라이 바지락죽/흑미밥 김칫국 비엔나구이*케찹 양...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 닭개장 돈육춘장볶음 김말이강정 꼬시래기무침 포기김치 양상추샐러...</td>\n",
       "      <td>흑미밥 근대국 코다리무조림 오꼬노미계란말이 상추무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>월</td>\n",
       "      <td>2973</td>\n",
       "      <td>125</td>\n",
       "      <td>174</td>\n",
       "      <td>704</td>\n",
       "      <td>331.0</td>\n",
       "      <td>모닝롤/커피콩빵 우유/주스 계란후라이 누룽지탕/흑미밥 청양콩나물국 스팸구이 양상추샐...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 쇠고기미역국 춘천닭갈비 오지치즈후라이 가지두반장볶음 포기김치 ...</td>\n",
       "      <td>흑미밥 돈육고추장찌개 갈치구이 김치전 취나물무침 깍두기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>화</td>\n",
       "      <td>2973</td>\n",
       "      <td>76</td>\n",
       "      <td>170</td>\n",
       "      <td>636</td>\n",
       "      <td>364.0</td>\n",
       "      <td>모닝롤/모닝샌드 우유/주스 계란후라이 고구마스프/흑미밥 아욱국 참치채소볶음 양상추샐...</td>\n",
       "      <td>쌀밥/귀리밥/찰현미밥 순두부백탕 매콤소갈비찜 깻잎완자전 돌나물초장무침 포기김치 시리...</td>\n",
       "      <td>추가밥 짬뽕*생면 수제찹쌀꿔바로우 메추리알곤약장조림 단무지무침 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>수</td>\n",
       "      <td>2973</td>\n",
       "      <td>96</td>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>334.0</td>\n",
       "      <td>모닝롤/호떡맥모닝 우유/주스 계란후라이 팥죽/흑미밥 닭살해장국 우엉채조림 양상추샐러...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 냉이국 돈육간장불고기 비빔냉면 오이나물볶음 겉절이김치 양상추샐...</td>\n",
       "      <td>단호박카레라이스 시금치된장국 소떡소떡 파프리카해초무침 감귤쥬스 포기김치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>목</td>\n",
       "      <td>2973</td>\n",
       "      <td>105</td>\n",
       "      <td>238</td>\n",
       "      <td>509</td>\n",
       "      <td>324.0</td>\n",
       "      <td>모닝롤/크로크무슈 우유/주스 계란후라이 누룽지탕/흑미밥 감자국 두부양념조림 양상추샐...</td>\n",
       "      <td>쌀밥/옥수수밥/찰현미밥 맑은떡국 (New)로제찜닭 가자미구이*장 유채나물무침 포기김...</td>\n",
       "      <td>흑미밥 어묵매운탕 쇠고기숙주볶음 채소계란찜 쑥갓생무침 김치볶음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>금</td>\n",
       "      <td>2973</td>\n",
       "      <td>259</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>모닝롤/토마토샌드 우유/주스 계란후라이 채소죽/흑미밥 대구지리 애호박나물볶음 양상추...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 사골우거지국 해물누룽지탕 청포묵*양념간장 비름나물고추장무침 석...</td>\n",
       "      <td>흑미밥 맑은버섯국 매운사태조림 춘권*타르타르D 열무나물무침 포기김치</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            일자 요일  본사정원수  본사휴가자수  본사출장자수  본사시간외근무명령서승인건수  현본사소속재택근무자수  \\\n",
       "0   2021-01-27  수   2983      88     182               5        358.0   \n",
       "1   2021-01-28  목   2983     104     212             409        348.0   \n",
       "2   2021-01-29  금   2983     270     249               0        294.0   \n",
       "3   2021-02-01  월   2924     108     154             538        322.0   \n",
       "4   2021-02-02  화   2924      62     186             455        314.0   \n",
       "5   2021-02-03  수   2924      59     199               5        286.0   \n",
       "6   2021-02-04  목   2924      61     211             476        288.0   \n",
       "7   2021-02-05  금   2924     169     252               0        256.0   \n",
       "8   2021-02-08  월   2924      88     174             690        329.0   \n",
       "9   2021-02-09  화   2924      94     183             542        329.0   \n",
       "10  2021-02-10  수   2924     489     134               0        233.0   \n",
       "11  2021-02-15  월   2924     178     131             795        355.0   \n",
       "12  2021-02-16  화   2924      70     175             815        413.0   \n",
       "13  2021-02-17  수   2924      77     181               3        379.0   \n",
       "14  2021-02-18  목   2924      83     247             594        338.0   \n",
       "15  2021-02-19  금   2924     176     268               1        280.0   \n",
       "16  2021-02-22  월   2924     105     197             814        247.0   \n",
       "17  2021-02-23  화   2924      75     200             783        233.0   \n",
       "18  2021-02-24  수   2924      77     235               3        234.0   \n",
       "19  2021-02-25  목   2924      91     252             585        235.0   \n",
       "20  2021-02-26  금   2924     261     279               1        179.0   \n",
       "21  2021-03-02  화   2975     139     166             781        248.0   \n",
       "22  2021-03-03  수   2975      50     195               1        292.0   \n",
       "23  2021-03-04  목   2975      72     236             746        263.0   \n",
       "24  2021-03-05  금   2975     158     257               2        226.0   \n",
       "25  2021-03-08  월   2975      97     170             939        264.0   \n",
       "26  2021-03-09  화   2975      76     170            1003        282.0   \n",
       "27  2021-03-10  수   2975      71     185               1        271.0   \n",
       "28  2021-03-11  목   2975      93     223             609        261.0   \n",
       "29  2021-03-12  금   2975     241     240               0        251.0   \n",
       "30  2021-03-15  월   2975     126     148             864        314.0   \n",
       "31  2021-03-16  화   2975      74     176             658        401.0   \n",
       "32  2021-03-17  수   2975      90     192               4        371.0   \n",
       "33  2021-03-18  목   2975     106     231             520        351.0   \n",
       "34  2021-03-19  금   2975     253     255               0        266.0   \n",
       "35  2021-03-22  월   2975     133     166             707        350.0   \n",
       "36  2021-03-23  화   2975      82     205             688        349.0   \n",
       "37  2021-03-24  수   2975      87     234               0        314.0   \n",
       "38  2021-03-25  목   2975     118     260             441        297.0   \n",
       "39  2021-03-26  금   2975     311     266               0        229.0   \n",
       "40  2021-03-29  월   2975     121     178             660        318.0   \n",
       "41  2021-03-30  화   2975      83     198             625        313.0   \n",
       "42  2021-03-31  수   2975      86     217               0        317.0   \n",
       "43  2021-04-01  목   2973      88     256             394        303.0   \n",
       "44  2021-04-02  금   2973     275     272               0        224.0   \n",
       "45  2021-04-05  월   2973     125     174             704        331.0   \n",
       "46  2021-04-06  화   2973      76     170             636        364.0   \n",
       "47  2021-04-07  수   2973      96     214               1        334.0   \n",
       "48  2021-04-08  목   2973     105     238             509        324.0   \n",
       "49  2021-04-09  금   2973     259     268               0        229.0   \n",
       "\n",
       "                                                 조식메뉴  \\\n",
       "0   모닝롤/연유버터베이글 우유/주스 계란후라이/찐계란 단호박죽/흑미밥 우거지국 고기완자...   \n",
       "1   모닝롤/대만샌드위치 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 황태국 시래기지짐 ...   \n",
       "2   모닝롤/핫케익 우유/주스 계란후라이/찐계란 오곡죽/흑미밥 매생이굴국 고구마순볶음 양...   \n",
       "3   모닝롤/촉촉한치즈케익 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 두부김칫국 새우완...   \n",
       "4   모닝롤/토마토샌드 우유/주스 계란후라이/찐계란 채소죽/흑미밥 호박맑은국 오이생채 양...   \n",
       "5   모닝롤/게살모닝샌드 우유/주스 계란후라이/찐계란 소고기죽/흑미밥 시래기된장국 베이컨...   \n",
       "6   모닝롤/사과파이 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 아욱국 새송이버섯곤약장...   \n",
       "7   모닝롤/앙버터모닝빵 우유/주스 계란후라이/찐계란 고구마죽/흑미밥 옹심이국 머위나물무...   \n",
       "8   모닝롤/스콘 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 꽃게탕 근대나물무침 연두부...   \n",
       "9   모닝롤/치즈팡샌드 우유/주스 계란후라이/찐계란 팥죽/흑미밥 맑은버섯국 시금치나물무침...   \n",
       "10  모닝롤/길거리토스트 우유/주스 계란후라이/찐계란 옥수수스프/흑미밥 우거지국 스팸구이...   \n",
       "11  모닝롤/파운드케익 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 열무된장국 분홍소세지...   \n",
       "12  모닝롤/모닝샌드 우유/주스 계란후라이/찐계란 흑임자죽/흑미밥 대구매운탕 가지나물 양...   \n",
       "13  모닝롤/트위스터버거 우유/주스 계란후라이/찐계란 단호박크림스프/흑미밥 사골파국 양념...   \n",
       "14  모닝롤/허니브레드 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 봄동된장국 참치채소볶...   \n",
       "15  모닝롤/크로와상샌드위치 우유/주스 계란후라이/찐계란 쇠고기죽/흑미밥 닭살해장국 해물...   \n",
       "16  모닝롤/흑미쌀찐빵 우유/주스 계란후라이 누룽지탕/흑미밥 미역국 매콤부들어묵볶음 양상...   \n",
       "17  모닝롤/프렌치토스트 우유/주스 계란후라이 녹두죽/흑미밥 북어해장국 부추김무침 양상추...   \n",
       "18  모닝롤/파게트 우유/주스 계란후라이 감자스프/흑미밥 시금치된장국 우엉조림 양상추샐러...   \n",
       "19  모닝롤/야채모닝샌드 우유/주스 계란후라이 누룽지탕/흑미밥 맑은버섯국 수제동그랑땡전*...   \n",
       "20  모닝롤/베이컨맥모닝 우유/주스 계란후라이 게살죽/흑미밥 매생이굴국 두부양념구이 양상...   \n",
       "21  모닝롤/호떡 우유/주스 계란후라이 누룽지탕/흑미밥 애호박새우젓국 재래김*양념장 양상...   \n",
       "22  모닝롤/크림치즈와플 우유/주스 계란후라이 고구마죽/흑미밥 사골우거지국 비엔나소세지볶...   \n",
       "23  모닝롤/BLT샌드 우유/주스 계란후라이 누룽지탕/흑미밥 아귀매운탕 동초나물무침 양상...   \n",
       "24  모닝롤/모닝사라다빵 우유/주스 계란후라이 낙지죽/흑미밥 감자국 멸치마늘종볶음 양상추...   \n",
       "25  모닝롤/프레즐 우유/주스 계란후라이 누룽지탕/흑미밥 맑은버섯국 스팸구이 양상추샐러드...   \n",
       "26  모닝롤/계란빵 우유/주스 계란후라이 채소죽/흑미밥 동태매운탕 모둠사태조림 양상추샐러...   \n",
       "27  모닝롤/모닝샌드 우유/주스 계란후라이 팥죽/흑미밥 북엇국 방풍나물 양상추샐러드/사과...   \n",
       "28  모닝롤/(New)당근크림치즈베이글 우유/주스 계란후라이 누룽지탕/흑미밥 조랭이떡국 ...   \n",
       "29  모닝롤/토마토샌드 우유/주스 계란후라이 참치죽/흑미밥 꽃게탕 섭산적채소조림 양상추샐...   \n",
       "30  모닝롤/고구마파이 우유/주스 계란후라이 누룽지탕/흑미밥 미역국 새우완자전*케찹 연두...   \n",
       "31  모닝롤/미니햄버거 우유/주스 계란후라이 오곡죽/흑미밥 대구지리 생깻잎지 양상추샐러드...   \n",
       "32  모닝롤/치즈볼 우유/주스 계란후라이 양송이스프/흑미밥 매운감자양파국 자반무침 양상추...   \n",
       "33  모닝롤/(New)밤크림빵 우유/주스 계란후라이 누룽지탕/흑미밥 콩나물오징어국 고사리...   \n",
       "34  모닝롤/인기가요샌드 우유/주스 계란후라이 흑임자죽/흑미밥 건새우아욱국 베이컨숙주볶음...   \n",
       "35  모닝롤/치즈케익 우유/주스 계란후라이 누룽지탕/흑미밥 시금치된장국 캔꽁치무조림 양상...   \n",
       "36  모닝롤/카야잼샌드 우유/주스 계란후라이 게살죽/흑미밥 들깨무채국 명엽채고추장볶음 양...   \n",
       "37  모닝롤/크로와상샌드 우유/주스 계란후라이 흑미두부죽/흑미밥 냉이된장국 쇠고기꽈리고추...   \n",
       "38  모닝롤/치아바타샌드 우유/주스 계란후라이 누룽지탕/흑미밥 쇠고기해장국 감자채볶음 양...   \n",
       "39  모닝롤/베이컨에그슬럿 우유/주스 계란후라이 단호박죽/흑미밥 아귀매운탕 땅콩조림 양상...   \n",
       "40  모닝롤/깨찰빵 우유/주스 계란후라이 누룽지탕/흑미밥 우거지국 마늘종호두조림 양상추샐...   \n",
       "41  모닝롤/대만샌드위치 우유/주스 계란후라이 채소새우죽/흑미밥 옹심이국 멸치고추장볶음 ...   \n",
       "42  모닝롤/모닝샌드 우유/주스 계란후라이 크루통크림스프/흑미밥 꽃게탕 건파래볶음 양상추...   \n",
       "43  모닝롤/단호박피자빵 우유/주스 계란후라이 누룽지탕/흑미밥 북어해장국 달래오이무침 양...   \n",
       "44  모닝롤/야채샌드위치 우유/주스 계란후라이 바지락죽/흑미밥 김칫국 비엔나구이*케찹 양...   \n",
       "45  모닝롤/커피콩빵 우유/주스 계란후라이 누룽지탕/흑미밥 청양콩나물국 스팸구이 양상추샐...   \n",
       "46  모닝롤/모닝샌드 우유/주스 계란후라이 고구마스프/흑미밥 아욱국 참치채소볶음 양상추샐...   \n",
       "47  모닝롤/호떡맥모닝 우유/주스 계란후라이 팥죽/흑미밥 닭살해장국 우엉채조림 양상추샐러...   \n",
       "48  모닝롤/크로크무슈 우유/주스 계란후라이 누룽지탕/흑미밥 감자국 두부양념조림 양상추샐...   \n",
       "49  모닝롤/토마토샌드 우유/주스 계란후라이 채소죽/흑미밥 대구지리 애호박나물볶음 양상추...   \n",
       "\n",
       "                                                 중식메뉴  \\\n",
       "0   쌀밥/흑미밥/찰현미밥 대구지리 매운돈갈비찜 오꼬노미계란말이 상추무침 포기김치 양상추...   \n",
       "1   쌀밥/보리밥/찰현미밥 우렁된장찌개 오리주물럭 청양부추전 수제삼색무쌈 겉절이김치 양상...   \n",
       "2   쌀밥/흑미밥/찰현미밥 팽이장국 수제돈까스*소스 가자미조림 동초나물무침 포기김치 양상...   \n",
       "3   쌀밥/흑미밥/찰현미밥 배추들깨국 오리대패불고기 시금치프리타타 부추고추장무침 포기김치...   \n",
       "4   쌀밥/팥밥/찰현미밥 부대찌개 닭살데리야끼조림 버섯탕수 세발나물무침 알타리김치/사과푸...   \n",
       "5    쌀밥/흑미밥/찰현미밥 아욱국 매콤해물볶음 감자조림 미나리나물 포기김치 콥샐러드*렌치D    \n",
       "6   쌀밥/차조밥/찰현미밥 설렁탕 고등어김치말이찜 볼어묵굴소스볶음 브로콜리숙회*초장 석박...   \n",
       "7   쌀밥/흑미밥/찰현미밥 북엇국 닭볶음탕 채소전*장 솎음열무나물무침 포기김치 양상추샐러...   \n",
       "8   쌀밥/흑미밥/찰현미밥 감자양파국 돈수육*씨앗쌈장 매콤어묵볶음 콩나물파채무침 포기김치...   \n",
       "9   쌀밥/기장밥/찰현미밥 장각백숙 적어양념장구이 채소스틱*쌈장 도라지오이초무침 겉절이김...   \n",
       "10  유니짜장밥 짬뽕국 수제찹쌀꿔바로우 계란후라이 단무지락교무침 포기김치 그린샐러드*딸기...   \n",
       "11  쌀밥/흑미밥/찰현미밥 떡국 소갈비찜 한식잡채 참나물겉절이 포기김치 양상추샐러드*블루...   \n",
       "12  쌀밥/수수밥/찰현미밥 육개장 닭살겨자냉채 오이스틱*쌈장 탕평채 깍두기/수박 양상추샐...   \n",
       "13  쌀밥/흑미밥/찰현미밥 미니쌀국수 삼겹살고추장구이 스프링롤*타르타르D 동초나물무침 알...   \n",
       "14  쌀밥/완두콩밥/찰현미밥 김치어묵탕 수원왕갈비통닭 두부양념조림 연근깨소스무침 포기김치...   \n",
       "15  쌀밥/흑미밥/찰현미밥 유부장국 해물누룽지탕 김치전 마약계란장조림 포기김치 양상추샐러...   \n",
       "16  쌀밥/흑미밥/찰현미밥 호박고추장찌개 안동찜닭 마카로니치즈범벅 세발나물무침 포기김치/...   \n",
       "17    쌀밥/보리밥/찰현미밥 근대국 등갈비김치찜 감자채전*장 치커리무침 깍두기 파스타샐러드    \n",
       "18  쌀밥/흑미밥/찰현미밥 해물탕 쇠고기숙주볶음 맛살계란말이 물미역초고추장무침 포기김치 ...   \n",
       "19  쌀밥/흑미밥/찰현미밥 나주곰탕 생선까스*타르타르D 더덕양념구이 방풍나물무침 석박지 ...   \n",
       "20  쌀밥/오곡밥/찰현미밥 옹심이국 목살스테이크 베이비크랩강정 이색나물(호박고지,건취나물...   \n",
       "21  쌀밥/흑미밥/찰현미밥 아욱국 치즈불닭 베이컨감자볶음 매운콩나물무침 포기김치 양배추샐...   \n",
       "22  쌀밥/흑미밥/찰현미밥 황태미역국 동파육 느타리버섯볶음 참나물상추겉절이 포기김치/망고...   \n",
       "23  쌀밥/옥수수밥/찰현미밥 매운쇠고기샤브샤브국 갈치조림 수수부꾸미 쑥갓두부무침 알타리김...   \n",
       "24  쌀밥/흑미밥/찰현미밥 쑥국 닭다리튀김 골뱅이채소무침 미나리나물 포기김치 양상추샐러드...   \n",
       "25  쌀밥/흑미밥/찰현미밥 얼갈이된장국 오리불고기 (New)순대탕수 깻잎무쌈 포기김치 양...   \n",
       "26  쌀밥/차조밥/찰현미밥 갈비탕 순살닭강정 매생이전 도라지오이생채 깍두기 양상추샐러드*...   \n",
       "27    봄나물비빔밥 냉이된장국 수제고기육전 도토리묵*양념장 쥬시쿨 포기김치 콥샐러드*렌치D    \n",
       "28  쌀밥/귀리밥/찰현미밥 콩가루배추국 타워함박스테이크 문어꽈리고추조림 시금치고추장나물무...   \n",
       "29  쌀밥/흑미밥/찰현미밥 어묵매운탕 목살구이 쌈채소*쌈장 부추무침 겉절이김치 양상추샐러...   \n",
       "30  쌀밥/흑미밥/찰현미밥 근대된장국 묵은지닭찜 비엔나브로콜리볶음 유부채소겨자냉채 깍두기...   \n",
       "31  쌀밥/수수밥/찰현미밥 대파육개장 고등어구이*와사비장 어묵잡채 건다래순볶음 포기김치 ...   \n",
       "32  쌀밥/흑미밥/찰현미밥 물만둣국 제육미나리볶음 두부까스*소스 아삭이고추된장무침 겉절이...   \n",
       "33  쌀밥/기장밥/찰현미밥 열무된장국 장각허브오븐구이*청양마요소스 수제오미산적 머위나물 ...   \n",
       "34  쌀밥/흑미밥/찰현미밥 버섯매운탕 돈갈비찜 (New)단호박계란찜 오이생채 포기김치 그...   \n",
       "35  쌀밥/흑미밥/찰현미밥 황태국 콩나물불고기 쇠고기납작당면볶음 삼색유자청무침 포기김치 ...   \n",
       "36  쌀밥/검정콩밥/찰현미밥 순남시래기국 장어강정*데리야끼소스 깻잎쌈*생강채 유채나물된장...   \n",
       "37  쌀밥/흑미밥/찰현미밥 미역국 바베큐폭립 건새우호박채전 비름나물 포기김치 양상추샐러드...   \n",
       "38  쌀밥/차조밥/찰현미밥 아욱국 짜파치킨 쫄면채소무침 취나물무침 포기김치 양배추샐러드*...   \n",
       "39  쌀밥/흑미밥/찰현미밥 돈육김치찌개 소불고기 가지나물 풋마늘대무침 깍두기 양상추샐러드...   \n",
       "40  쌀밥/흑미밥/찰현미밥 동태매운탕 차돌박이구이&청경채찜 메추리알떡볶이 세발나물무침 포...   \n",
       "41  쌀밥/완두콩밥/찰현미밥 유부장국 돈수육 브로콜리땅콩소스무침 모듬채소*쌈장 수제보쌈김...   \n",
       "42  쌀밥/흑미밥/찰현미밥 콩가루배추국 허니순살치킨 버섯초장무침 방풍나물 포기김치 양상추...   \n",
       "43  쌀밥/보리밥/찰현미밥 순대국밥*다대기 해물동그랑땡채소볶음 통들깨부추무침 채소스틱 석...   \n",
       "44  쌀밥/흑미밥/찰현미밥 닭개장 돈육춘장볶음 김말이강정 꼬시래기무침 포기김치 양상추샐러...   \n",
       "45  쌀밥/흑미밥/찰현미밥 쇠고기미역국 춘천닭갈비 오지치즈후라이 가지두반장볶음 포기김치 ...   \n",
       "46  쌀밥/귀리밥/찰현미밥 순두부백탕 매콤소갈비찜 깻잎완자전 돌나물초장무침 포기김치 시리...   \n",
       "47  쌀밥/흑미밥/찰현미밥 냉이국 돈육간장불고기 비빔냉면 오이나물볶음 겉절이김치 양상추샐...   \n",
       "48  쌀밥/옥수수밥/찰현미밥 맑은떡국 (New)로제찜닭 가자미구이*장 유채나물무침 포기김...   \n",
       "49  쌀밥/흑미밥/찰현미밥 사골우거지국 해물누룽지탕 청포묵*양념간장 비름나물고추장무침 석...   \n",
       "\n",
       "                                                 석식메뉴  \n",
       "0       흑미밥 얼큰순두부찌개 쇠고기우엉볶음 버섯햄볶음 (New)아삭이고추무절임 포기김치   \n",
       "1                 충무김밥 우동국물 오징어무침 꽃맛살샐러드 얼갈이쌈장무침 석박지   \n",
       "2                 흑미밥 물만둣국 카레찜닭 숯불양념꼬지어묵 꼬시래기무침 포기김치   \n",
       "3                흑미밥 동태탕 돈육꽈리고추장조림 당면채소무침 모자반무침 포기김치   \n",
       "4            흑미밥 바지락살국 쇠고기청경채볶음 두부구이*볶은김치 머위된장무침 백김치   \n",
       "5        오므라이스 가쓰오장국 빌소세지구이*구운채소 단감치커리무침 양념고추지 겉절이김치   \n",
       "6          흑미밥 계란파국 돈육두루치기 감자채파프리카볶음 세발나물오리엔탈무침 포기김치   \n",
       "7            유부초밥/추가밥 온메밀소바 국물떡볶이 순대찜*소금 청경채겉절이 포기김치   \n",
       "8                      흑미밥 냉이국 반반치킨 꼬막채소무침 청경채찜 포기김치   \n",
       "9                 흑미밥 미역국 매운소불고기 단호박두부탕수 메추리알장조림 석박지   \n",
       "10            흑미밥 참치김치찌개 오징어굴소스볶음 차돌비빔국수 건새우무나물 포기김치   \n",
       "11                흑미밥 순두부백탕 수제치킨까스 쫄면채소무침 얼갈이나물 포기김치   \n",
       "12                 흑미밥 손수제비국 쇠고기낙지볶음 카레홍합찜 쑥갓나물 포기김치   \n",
       "13              곤드레밥 황태국 찰떡떡갈비조림 계란후라이 재래김*달래양념장 무생채   \n",
       "14            흑미밥 바지락된장찌개 제육볶음 양배추숙*쌈장 노가리고추조림 겉절이김치   \n",
       "15            흑미밥 버섯들깨탕 아귀콩나물찜 콤비네이션피자 돌나물&된장소스 포기김치   \n",
       "16                      흑미밥 동태알탕 깐풍육 고사리볶음 오이무침 포기김치   \n",
       "17               흑미밥 쇠고기무국 춘전닭갈비 뉴욕핫도그 유채나물된장무침 포기김치   \n",
       "18                 애플카레라이스 팽이장국 가지탕수 소떡소떡 오복지무침 포기김치   \n",
       "19                흑미밥 계란파국 쭈꾸미불고기 모둠채소전*장 씨앗콩자반 포기김치   \n",
       "20          흑미밥 스팸김치찌개 삼치구이*와사비장 브로콜리깨소스무침 연근조림 포기김치   \n",
       "21               흑미밥 냉이김칫국 해물우동볶음 날치알계란찜 솎음열무나물 포기김치   \n",
       "22      흑미밥 (New)수제오떡탕 매운족발볶음 크래미오이보트샐러드 청경채나물 겉절이김치   \n",
       "23                  흑미밥 짬뽕국 쇠고기탕수 고추잡채*꽃빵 해초배무침 포기김치   \n",
       "24         샐러드김밥 미소시루 라볶이 상추튀김(모둠튀김*양념장) 단무지채무침 포기김치   \n",
       "25                 흑미밥 달래된장찌개 코코뱅 고구마치즈구이 치커리무침 포기김치   \n",
       "26            흑미밥 맑은콩나물국 돈육김치볶음 수제두부동그랑땡 유채나물무침 포기김치   \n",
       "27                흑미밥 순두부백탕 낙지볶음 쇠고기들깨소스무침 쪽파무침 포기김치   \n",
       "28    꽁보리밥*볶음고추장 닭칼국수 왕만두찜*양념장 버섯맛살볶음 양파장아찌 얼갈이열무겉절이   \n",
       "29               흑미밥 뼈해장국 가자미유린기 매운감자조림 배추흑임자무침 포기김치   \n",
       "30                해물짜장면 계란파국 사천탕수육 세발나물무침 짜사이볶음 포기김치   \n",
       "31      흑미밥 사골파국*소면사리 매콤돈육메추리알장조림 명엽채볶음 참나물초장무침 포기김치   \n",
       "32          흑미밥 김칫국 닭살채소굴소스볶음 애호박나물 알배기,케일숙쌈*쌈장 포기김치   \n",
       "33       쇠고기규동덮밥 가쓰오장국 (New)피자핫도그 연근땅콩조림 청경채겉절이 포기김치   \n",
       "34                   흑미밥 부대찌개 삼치엿장구이 부추전 치커리사과무침 무생채   \n",
       "35               흑미밥 순두부찌개 닭간장조림 매콤어묵볶음 미나리숙주나물 포기김치   \n",
       "36            베이컨김치볶음밥 우동국물 알리오올리오 계란후라이 수제오이피클 포기김치   \n",
       "37         흑미밥 맑은버섯국 오삼불고기 양상추, 쇠미역쌈*강된장 고들빼기무침 포기김치   \n",
       "38               흑미밥 짬뽕수제비 생선까스*타르타르D 더덕무침 쑥갓무침 포기김치   \n",
       "39       흑미밥/미니팥칼국수 차돌된장찌개 적어양념구이 미트볼채소볶음 오복지무침 포기김치   \n",
       "40           셀프충무김밥 미소시루 오징어어묵무침 콩나물간장볶음 꽃맛살샐러드 포기김치   \n",
       "41  오므라이스/추가밥 애호박새우젓국 빌소세지구이*구운채소 도토리묵채소무침 수제연근유자피...  \n",
       "42             흑미밥 쇠고기무국 마파두부소스 납작군만두*장 참나물생채무침 포기김치   \n",
       "43                 흑미밥 미니우동 치킨까스김치나베 미나리전 양념고추지 열무김치   \n",
       "44                 흑미밥 근대국 코다리무조림 오꼬노미계란말이 상추무침 포기김치   \n",
       "45                    흑미밥 돈육고추장찌개 갈치구이 김치전 취나물무침 깍두기   \n",
       "46           추가밥 짬뽕*생면 수제찹쌀꿔바로우 메추리알곤약장조림 단무지무침 포기김치   \n",
       "47           단호박카레라이스 시금치된장국 소떡소떡 파프리카해초무침 감귤쥬스 포기김치   \n",
       "48                흑미밥 어묵매운탕 쇠고기숙주볶음 채소계란찜 쑥갓생무침 김치볶음   \n",
       "49             흑미밥 맑은버섯국 매운사태조림 춘권*타르타르D 열무나물무침 포기김치   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f70bf5-4ca9-4df7-9771-69ab113ae98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>요일</th>\n",
       "      <th>본사정원수</th>\n",
       "      <th>본사휴가자수</th>\n",
       "      <th>본사출장자수</th>\n",
       "      <th>본사시간외근무명령서승인건수</th>\n",
       "      <th>현본사소속재택근무자수</th>\n",
       "      <th>조식메뉴</th>\n",
       "      <th>중식메뉴</th>\n",
       "      <th>석식메뉴</th>\n",
       "      <th>중식계</th>\n",
       "      <th>석식계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>월</td>\n",
       "      <td>2601</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "      <td>238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/찐빵  우유/두유/주스 계란후라이  호두죽/쌀밥 (쌀:국내산) 된장찌개  쥐...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 오징어찌개  쇠불고기 (쇠고기:호주산) 계란찜 ...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 육개장  자반고등어구이  두부조림  건파래무침 ...</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-02</td>\n",
       "      <td>화</td>\n",
       "      <td>2601</td>\n",
       "      <td>50</td>\n",
       "      <td>173</td>\n",
       "      <td>319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/단호박샌드  우유/두유/주스 계란후라이  팥죽/쌀밥 (쌀:국내산) 호박젓국찌...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 김치찌개  가자미튀김  모둠소세지구이  마늘쫑무...</td>\n",
       "      <td>콩나물밥*양념장 (쌀,현미흑미:국내산) 어묵국  유산슬 (쇠고기:호주산) 아삭고추무...</td>\n",
       "      <td>867.0</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>수</td>\n",
       "      <td>2601</td>\n",
       "      <td>56</td>\n",
       "      <td>180</td>\n",
       "      <td>111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/베이글  우유/두유/주스 계란후라이  표고버섯죽/쌀밥 (쌀:국내산) 콩나물국...</td>\n",
       "      <td>카레덮밥 (쌀,현미흑미:국내산) 팽이장국  치킨핑거 (닭고기:국내산) 쫄면야채무침 ...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 청국장찌개  황태양념구이 (황태:러시아산) 고기...</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-04</td>\n",
       "      <td>목</td>\n",
       "      <td>2601</td>\n",
       "      <td>104</td>\n",
       "      <td>220</td>\n",
       "      <td>355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/토마토샌드  우유/두유/주스 계란후라이  닭죽/쌀밥 (쌀,닭:국내산) 근대국...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 쇠고기무국  주꾸미볶음  부추전  시금치나물  ...</td>\n",
       "      <td>미니김밥*겨자장 (쌀,현미흑미:국내산) 우동  멕시칸샐러드  군고구마  무피클  포...</td>\n",
       "      <td>978.0</td>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>금</td>\n",
       "      <td>2601</td>\n",
       "      <td>278</td>\n",
       "      <td>181</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/와플  우유/두유/주스 계란후라이  쇠고기죽/쌀밥 (쌀:국내산) 재첩국  방...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 떡국  돈육씨앗강정 (돼지고기:국내산) 우엉잡채...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 차돌박이찌개 (쇠고기:호주산) 닭갈비 (닭고기:...</td>\n",
       "      <td>925.0</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자 요일  본사정원수  본사휴가자수  본사출장자수  본사시간외근무명령서승인건수  현본사소속재택근무자수  \\\n",
       "0  2016-02-01  월   2601      50     150             238          0.0   \n",
       "1  2016-02-02  화   2601      50     173             319          0.0   \n",
       "2  2016-02-03  수   2601      56     180             111          0.0   \n",
       "3  2016-02-04  목   2601     104     220             355          0.0   \n",
       "4  2016-02-05  금   2601     278     181              34          0.0   \n",
       "\n",
       "                                                조식메뉴  \\\n",
       "0  모닝롤/찐빵  우유/두유/주스 계란후라이  호두죽/쌀밥 (쌀:국내산) 된장찌개  쥐...   \n",
       "1  모닝롤/단호박샌드  우유/두유/주스 계란후라이  팥죽/쌀밥 (쌀:국내산) 호박젓국찌...   \n",
       "2  모닝롤/베이글  우유/두유/주스 계란후라이  표고버섯죽/쌀밥 (쌀:국내산) 콩나물국...   \n",
       "3  모닝롤/토마토샌드  우유/두유/주스 계란후라이  닭죽/쌀밥 (쌀,닭:국내산) 근대국...   \n",
       "4  모닝롤/와플  우유/두유/주스 계란후라이  쇠고기죽/쌀밥 (쌀:국내산) 재첩국  방...   \n",
       "\n",
       "                                                중식메뉴  \\\n",
       "0  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 오징어찌개  쇠불고기 (쇠고기:호주산) 계란찜 ...   \n",
       "1  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 김치찌개  가자미튀김  모둠소세지구이  마늘쫑무...   \n",
       "2  카레덮밥 (쌀,현미흑미:국내산) 팽이장국  치킨핑거 (닭고기:국내산) 쫄면야채무침 ...   \n",
       "3  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 쇠고기무국  주꾸미볶음  부추전  시금치나물  ...   \n",
       "4  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 떡국  돈육씨앗강정 (돼지고기:국내산) 우엉잡채...   \n",
       "\n",
       "                                                석식메뉴     중식계    석식계  \n",
       "0  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 육개장  자반고등어구이  두부조림  건파래무침 ...  1039.0  331.0  \n",
       "1  콩나물밥*양념장 (쌀,현미흑미:국내산) 어묵국  유산슬 (쇠고기:호주산) 아삭고추무...   867.0  560.0  \n",
       "2  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 청국장찌개  황태양념구이 (황태:러시아산) 고기...  1017.0  573.0  \n",
       "3  미니김밥*겨자장 (쌀,현미흑미:국내산) 우동  멕시칸샐러드  군고구마  무피클  포...   978.0  525.0  \n",
       "4  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 차돌박이찌개 (쇠고기:호주산) 닭갈비 (닭고기:...   925.0  330.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16969ee7-224a-498e-8f5b-5c2cd958fa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수',\n",
       "       '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴', '중식계', '석식계'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07720f17-ad5e-43bf-9341-738e4e712de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1553b917-1f3e-4c92-938c-ae0c77a9e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240606_152445\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 3600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240606_152445/ds_sub_fit/sub_fit_ho.\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240606_152445/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       11.33 GB / 31.92 GB (35.5%)\n",
      "Disk Space Avail:   191.55 GB / 475.62 GB (40.3%)\n",
      "===================================================\n",
      "Train Data Rows:    1071\n",
      "Train Data Columns: 11\n",
      "Label Column:       중식계\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11605.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.83 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 316\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 2 | ['현본사소속재택근무자수', '석식계']\n",
      "\t\t('int', [])                        : 4 | ['본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수']\n",
      "\t\t('object', [])                     : 1 | ['요일']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['일자']\n",
      "\t\t('object', ['text'])               : 3 | ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   1 | ['요일']\n",
      "\t\t('category', ['text_as_category'])  :   3 | ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\t\t('float', [])                       :   2 | ['현본사소속재택근무자수', '석식계']\n",
      "\t\t('int', [])                         :   4 | ['본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수']\n",
      "\t\t('int', ['binned', 'text_special']) :  43 | ['조식메뉴.char_count', '조식메뉴.word_count', '조식메뉴.capital_ratio', '조식메뉴.lower_ratio', '조식메뉴.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :   5 | ['일자', '일자.year', '일자.month', '일자.day', '일자.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 249 | ['__nlp__.new', '__nlp__.가지나물', '__nlp__.가지나물 포기김치', '__nlp__.가지나물 포기김치 김치', '__nlp__.가쯔오장국', ...]\n",
      "\t3.8s = Fit runtime\n",
      "\t11 features in original data used to generate 307 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.86s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 597.28s of the 896.13s of remaining time.\n",
      "\t-246.7733\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 594.1s of the 892.95s of remaining time.\n",
      "\t-218.2133\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 593.99s of the 892.84s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 593.79s of the 892.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 593.62s of the 892.47s of remaining time.\n",
      "\t-89.9767\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.91s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 591.51s of the 890.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 591.35s of the 890.21s of remaining time.\n",
      "\t-96.3549\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 590.16s of the 889.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 590.04s of the 888.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 589.84s of the 888.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-96.507\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.03s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 538.6s of the 837.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 538.41s of the 837.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 538.25s of the 837.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-89.8816\t = Validation score   (-root_mean_squared_error)\n",
      "\t68.03s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 469.99s of the 768.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 469.78s of the 768.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 469.61s of the 768.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 469.43s of the 768.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 469.25s of the 768.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-91.1578\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.66s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 407.37s of the 706.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 407.1s of the 705.95s of remaining time.\n",
      "\t-98.1637\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 405.95s of the 704.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 405.74s of the 704.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 405.58s of the 704.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 405.41s of the 704.27s of remaining time.\n",
      "\t-89.2061\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 404.14s of the 702.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 403.96s of the 702.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 403.83s of the 702.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 403.63s of the 702.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-91.4066\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.29s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 336.11s of the 634.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 335.91s of the 634.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-92.3616\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.4s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 294.29s of the 593.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 294.1s of the 592.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 293.95s of the 592.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 293.78s of the 592.63s of remaining time.\n",
      "\t-97.9777\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 293.11s of the 591.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 292.89s of the 591.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 292.75s of the 591.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-93.6527\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.29s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 275.27s of the 574.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 275.07s of the 573.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 274.93s of the 573.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 274.74s of the 573.6s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 274.59s of the 573.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 274.39s of the 573.25s of remaining time.\n",
      "\t-89.7647\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.85s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 273.37s of the 572.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 273.19s of the 572.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 273.05s of the 571.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-90.8769\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.44s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 236.39s of the 535.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r98_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 236.18s of the 535.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r15_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 236.01s of the 534.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-91.0437\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.82s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 161.95s of the 460.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r86_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 161.68s of the 460.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r37_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 161.5s of the 460.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-95.2051\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.94s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 143.36s of the 442.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r49_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 143.17s of the 442.02s of remaining time.\n",
      "\t-116.8546\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 142.47s of the 441.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r143_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 142.28s of the 441.14s of remaining time.\n",
      "\t-91.2157\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 141.43s of the 440.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r134_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 141.3s of the 440.15s of remaining time.\n",
      "\t-112.6548\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 140.71s of the 439.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r94_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 140.52s of the 439.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 68)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 69)\n",
      "\t-90.5172\t = Validation score   (-root_mean_squared_error)\n",
      "\t115.36s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 24.9s of the 323.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r128_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 24.7s of the 323.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r111_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 24.56s of the 323.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 29)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 28)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 30)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 32)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 33)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 35)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 39)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 47)\n",
      "\t-92.1913\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.36s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 0.98s of the 299.84s of remaining time.\n",
      "\t-116.4923\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 0.27s of the 299.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r65_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 0.1s of the 298.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r88_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 298.57s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForest_r195_BAG_L1': 0.281, 'NeuralNetTorch_r79_BAG_L1': 0.202, 'NeuralNetTorch_r158_BAG_L1': 0.202, 'NeuralNetTorch_r41_BAG_L1': 0.169, 'NeuralNetTorch_r143_BAG_L1': 0.09, 'NeuralNetTorch_r22_BAG_L1': 0.034, 'KNeighborsDist_BAG_L1': 0.022}\n",
      "\t-84.2478\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 298.27s of the 298.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 298.07s of the 298.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 297.89s of the 297.82s of remaining time.\n",
      "\t-85.2073\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 295.83s of the 295.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 295.64s of the 295.57s of remaining time.\n",
      "\t-86.1096\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 294.28s of the 294.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 294.13s of the 294.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 293.91s of the 293.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.1115\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.56s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 267.14s of the 267.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 266.93s of the 266.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 266.75s of the 266.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.5357\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.22s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 218.3s of the 218.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 218.09s of the 218.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 217.94s of the 217.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 217.74s of the 217.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 217.56s of the 217.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-85.7013\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.07s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 176.26s of the 176.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 176.01s of the 175.94s of remaining time.\n",
      "\t-85.4558\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 174.86s of the 174.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 174.67s of the 174.6s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 174.51s of the 174.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 174.32s of the 174.25s of remaining time.\n",
      "\t-85.4387\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 172.65s of the 172.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 172.45s of the 172.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 172.31s of the 172.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 172.1s of the 172.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.2984\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.31s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 127.54s of the 127.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 127.31s of the 127.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.3961\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.89s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L2 ... Training model for up to 94.2s of the 94.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L2 ... Training model for up to 93.97s of the 93.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r194_BAG_L2 ... Training model for up to 93.8s of the 93.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r172_BAG_L2 ... Training model for up to 93.58s of the 93.51s of remaining time.\n",
      "\t-85.4822\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L2 ... Training model for up to 92.81s of the 92.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L2 ... Training model for up to 92.62s of the 92.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r14_BAG_L2 ... Training model for up to 92.46s of the 92.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-89.0215\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.1s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L2 ... Training model for up to 77.11s of the 77.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L2 ... Training model for up to 76.88s of the 76.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r70_BAG_L2 ... Training model for up to 76.7s of the 76.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L2 ... Training model for up to 76.5s of the 76.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r196_BAG_L2 ... Training model for up to 76.32s of the 76.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r39_BAG_L2 ... Training model for up to 76.13s of the 76.06s of remaining time.\n",
      "\t-85.4764\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L2 ... Training model for up to 74.73s of the 74.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L2 ... Training model for up to 74.5s of the 74.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r41_BAG_L2 ... Training model for up to 74.33s of the 74.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.7603\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.11s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L2 ... Training model for up to 46.97s of the 46.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r98_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: LightGBM_r15_BAG_L2 ... Training model for up to 46.75s of the 46.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r15_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L2 ... Training model for up to 46.56s of the 46.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 30)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 33)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 30)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 32)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 33)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 39)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 44)\n",
      "\t-87.2804\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.68s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 4.86s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L2': 0.33, 'NeuralNetTorch_r22_BAG_L2': 0.237, 'NeuralNetTorch_r41_BAG_L2': 0.175, 'NeuralNetTorch_r30_BAG_L2': 0.103, 'ExtraTrees_r42_BAG_L2': 0.072, 'RandomForest_r195_BAG_L1': 0.062, 'NeuralNetTorch_r158_BAG_L2': 0.021}\n",
      "\t-83.6521\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 895.46s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240606_152445/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                         model  holdout_score   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    NeuralNetTorch_r22_BAG_L2     -92.298444  -85.701274  root_mean_squared_error        4.141829       3.288879  623.150563                 0.290697                0.151385          41.065073            2       True         29\n",
      "1    NeuralNetTorch_r79_BAG_L2     -92.321721  -87.535736  root_mean_squared_error        4.132286       3.282349  630.304894                 0.281154                0.144855          48.219404            2       True         28\n",
      "2   NeuralNetTorch_r158_BAG_L2     -92.930318  -87.280431  root_mean_squared_error        4.097588       3.296797  622.770378                 0.246456                0.159303          40.684888            2       True         38\n",
      "3          WeightedEnsemble_L3     -93.389876  -83.652147  root_mean_squared_error        5.135196       4.012876  738.321080                 0.007007                0.000997           0.285929            3       True         39\n",
      "4    NeuralNetTorch_r86_BAG_L2     -93.661891  -87.396148  root_mean_squared_error        4.096008       3.283897  614.973819                 0.244876                0.146403          32.888328            2       True         33\n",
      "5        NeuralNetTorch_BAG_L2     -94.072905  -87.111458  root_mean_squared_error        4.128479       3.271153  608.641972                 0.277347                0.133659          26.556481            2       True         27\n",
      "6    NeuralNetTorch_r41_BAG_L2     -94.122496  -86.760309  root_mean_squared_error        4.069255       3.290013  609.192884                 0.218123                0.152519          27.107394            2       True         37\n",
      "7    NeuralNetTorch_r30_BAG_L2     -94.256927  -87.298350  root_mean_squared_error        4.156114       3.289062  626.394076                 0.304982                0.151568          44.308586            2       True         32\n",
      "8     RandomForest_r195_BAG_L1     -95.222463  -89.206065  root_mean_squared_error        0.098948       0.126218    1.069924                 0.098948                0.126218           1.069924            1       True          9\n",
      "9    NeuralNetTorch_r14_BAG_L2     -95.443433  -89.021494  root_mean_squared_error        4.105967       3.302925  597.182496                 0.254835                0.165431          15.097005            2       True         35\n",
      "10     RandomForest_r39_BAG_L2     -95.502215  -85.476400  root_mean_squared_error        3.918622       3.263105  583.262818                 0.067490                0.125611           1.177328            2       True         36\n",
      "11      RandomForestMSE_BAG_L1     -95.507560  -89.976711  root_mean_squared_error        0.119520       0.124166    1.913478                 0.119520                0.124166           1.913478            1       True          3\n",
      "12     RandomForest_r39_BAG_L1     -95.511736  -89.764692  root_mean_squared_error        0.086420       0.115339    0.845631                 0.086420                0.115339           0.845631            1       True         14\n",
      "13      ExtraTrees_r172_BAG_L2     -95.518276  -85.482170  root_mean_squared_error        3.922116       3.271974  582.657955                 0.070984                0.134480           0.572465            2       True         34\n",
      "14      RandomForestMSE_BAG_L2     -96.024330  -85.207253  root_mean_squared_error        3.969380       3.276589  583.922866                 0.118248                0.139095           1.837376            2       True         25\n",
      "15    RandomForest_r195_BAG_L2     -96.063350  -85.438746  root_mean_squared_error        3.946235       3.257898  583.552505                 0.095103                0.120404           1.467015            2       True         31\n",
      "16    RandomForest_r127_BAG_L1     -96.100012  -91.215698  root_mean_squared_error        0.067965       0.111029    0.679884                 0.067965                0.111029           0.679884            1       True         19\n",
      "17       ExtraTrees_r42_BAG_L2     -96.199651  -85.455833  root_mean_squared_error        3.949683       3.258009  583.031834                 0.098551                0.120515           0.946344            2       True         30\n",
      "18        ExtraTreesMSE_BAG_L2     -96.637907  -86.109584  root_mean_squared_error        3.945688       3.268071  583.231251                 0.094556                0.130577           1.145760            2       True         26\n",
      "19         WeightedEnsemble_L2     -96.855795  -84.247754  root_mean_squared_error        1.557375       0.893551  356.695550                 0.010016                0.000000           0.275275            2       True         24\n",
      "20        ExtraTreesMSE_BAG_L1     -97.719017  -96.354852  root_mean_squared_error        0.091417       0.125371    0.993822                 0.091417                0.125371           0.993822            1       True          4\n",
      "21      ExtraTrees_r172_BAG_L1     -97.787218  -97.977675  root_mean_squared_error        0.072048       0.108148    0.511987                 0.072048                0.108148           0.511987            1       True         12\n",
      "22  NeuralNetTorch_r158_BAG_L1     -99.381679  -91.043682  root_mean_squared_error        0.321425       0.138496   73.822537                 0.321425                0.138496          73.822537            1       True         16\n",
      "23       ExtraTrees_r42_BAG_L1     -99.593110  -98.163679  root_mean_squared_error        0.118568       0.136388    0.941141                 0.118568                0.136388           0.941141            1       True          8\n",
      "24   NeuralNetTorch_r79_BAG_L1    -101.321226  -89.881571  root_mean_squared_error        0.270389       0.138274   68.034652                 0.270389                0.138274          68.034652            1       True          6\n",
      "25   NeuralNetTorch_r30_BAG_L1    -101.402337  -91.406554  root_mean_squared_error        0.252538       0.151731   67.285158                 0.252538                0.151731          67.285158            1       True         10\n",
      "26   NeuralNetTorch_r41_BAG_L1    -101.526074  -90.876932  root_mean_squared_error        0.266506       0.141317   36.441326                 0.266506                0.141317          36.441326            1       True         15\n",
      "27  NeuralNetTorch_r143_BAG_L1    -103.465156  -90.517176  root_mean_squared_error        0.312097       0.168280  115.360164                 0.312097                0.168280         115.360164            1       True         21\n",
      "28   NeuralNetTorch_r14_BAG_L1    -103.889446  -93.652727  root_mean_squared_error        0.243623       0.131076   17.289148                 0.243623                0.131076          17.289148            1       True         13\n",
      "29   NeuralNetTorch_r22_BAG_L1    -104.957725  -91.157837  root_mean_squared_error        0.235391       0.143033   61.663654                 0.235391                0.143033          61.663654            1       True          7\n",
      "30  NeuralNetTorch_r197_BAG_L1    -106.026955  -95.205074  root_mean_squared_error        0.221386       0.140538   17.938506                 0.221386                0.140538          17.938506            1       True         17\n",
      "31     RandomForest_r34_BAG_L1    -106.976924 -112.654753  root_mean_squared_error        0.055443       0.101683    0.439915                 0.055443                0.101683           0.439915            1       True         20\n",
      "32   NeuralNetTorch_r31_BAG_L1    -107.917408  -92.191330  root_mean_squared_error        0.236145       0.149008   23.356080                 0.236145                0.149008          23.356080            1       True         22\n",
      "33        ExtraTrees_r4_BAG_L1    -108.729336 -116.492276  root_mean_squared_error        0.070397       0.130964    0.518782                 0.070397                0.130964           0.518782            1       True         23\n",
      "34       NeuralNetTorch_BAG_L1    -108.875909  -96.507032  root_mean_squared_error        0.273710       0.135421   51.034699                 0.273710                0.135421          51.034699            1       True          5\n",
      "35   NeuralNetTorch_r86_BAG_L1    -108.955435  -92.361625  root_mean_squared_error        0.241954       0.147335   41.396408                 0.241954                0.147335          41.396408            1       True         11\n",
      "36       ExtraTrees_r49_BAG_L1    -115.757807 -116.854633  root_mean_squared_error        0.098677       0.121816    0.496655                 0.098677                0.121816           0.496655            1       True         18\n",
      "37       KNeighborsDist_BAG_L1    -219.882432 -218.213320  root_mean_squared_error        0.042602       0.037932    0.028019                 0.042602                0.037932           0.028019            1       True          2\n",
      "38       KNeighborsUnif_BAG_L1    -245.785088 -246.773252  root_mean_squared_error        0.053960       0.313931    0.023920                 0.053960                0.313931           0.023920            1       True          1\n",
      "Stacked overfitting occurred: False.\n",
      "Spend 902 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 2698 seconds.\n",
      "Starting full fit now with num_stack_levels 1.\n",
      "Beginning AutoGluon training ... Time limit = 2698s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240606_152445\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       10.23 GB / 31.92 GB (32.0%)\n",
      "Disk Space Avail:   191.47 GB / 475.62 GB (40.3%)\n",
      "===================================================\n",
      "Train Data Rows:    1205\n",
      "Train Data Columns: 11\n",
      "Label Column:       중식계\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10474.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.39 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 384\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 2 | ['현본사소속재택근무자수', '석식계']\n",
      "\t\t('int', [])                        : 4 | ['본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수']\n",
      "\t\t('object', [])                     : 1 | ['요일']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['일자']\n",
      "\t\t('object', ['text'])               : 3 | ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   1 | ['요일']\n",
      "\t\t('category', ['text_as_category'])  :   3 | ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\t\t('float', [])                       :   2 | ['현본사소속재택근무자수', '석식계']\n",
      "\t\t('int', [])                         :   4 | ['본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수']\n",
      "\t\t('int', ['binned', 'text_special']) :  43 | ['조식메뉴.char_count', '조식메뉴.word_count', '조식메뉴.capital_ratio', '조식메뉴.lower_ratio', '조식메뉴.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :   5 | ['일자', '일자.year', '일자.month', '일자.day', '일자.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 301 | ['__nlp__.new', '__nlp__.가지나물', '__nlp__.가지나물 포기김치', '__nlp__.가지나물 포기김치 김치', '__nlp__.가쯔오장국', ...]\n",
      "\t5.2s = Fit runtime\n",
      "\t11 features in original data used to generate 359 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.85 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1794.75s of the 2692.79s of remaining time.\n",
      "\t-248.077\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1794.63s of the 2692.67s of remaining time.\n",
      "\t-214.6408\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1794.49s of the 2692.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1794.28s of the 2692.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1794.07s of the 2692.11s of remaining time.\n",
      "\t-90.1295\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1792.25s of the 2690.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1792.06s of the 2690.1s of remaining time.\n",
      "\t-95.5664\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1790.49s of the 2688.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1790.35s of the 2688.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1790.14s of the 2688.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-95.8169\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.47s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1755.45s of the 2653.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1755.21s of the 2653.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1755.01s of the 2653.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-92.1044\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.66s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1723.13s of the 2621.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1722.89s of the 2620.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1722.74s of the 2620.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1722.53s of the 2620.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1722.34s of the 2620.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-90.9969\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.64s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 1672.47s of the 2570.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 1672.15s of the 2570.19s of remaining time.\n",
      "\t-96.7545\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1670.99s of the 2569.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 1670.79s of the 2568.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 1670.64s of the 2568.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 1670.45s of the 2568.49s of remaining time.\n",
      "\t-89.6673\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 1668.99s of the 2567.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 1668.79s of the 2566.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 1668.64s of the 2566.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 1668.42s of the 2566.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-91.8778\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.13s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 1618.06s of the 2516.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 1617.76s of the 2515.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-95.4841\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.27s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 1591.29s of the 2489.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 1591.06s of the 2489.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 1590.91s of the 2488.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 1590.72s of the 2488.76s of remaining time.\n",
      "\t-96.1697\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 1589.97s of the 2488.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 1589.75s of the 2487.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 1589.61s of the 2487.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-95.8286\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.44s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 1577.98s of the 2476.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 1577.75s of the 2475.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 1577.59s of the 2475.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 1577.39s of the 2475.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 1577.23s of the 2475.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 1577.02s of the 2475.06s of remaining time.\n",
      "\t-89.6438\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 1575.95s of the 2473.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 1575.76s of the 2473.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 1575.62s of the 2473.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-94.0966\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.98s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 1557.43s of the 2455.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r98_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 1557.21s of the 2455.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r15_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 1556.99s of the 2455.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-93.5778\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.71s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 1524.05s of the 2422.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r86_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 1523.81s of the 2421.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r37_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 1523.65s of the 2421.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-92.5668\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.6s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 1510.87s of the 2408.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r49_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 1510.65s of the 2408.69s of remaining time.\n",
      "\t-119.3834\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 1509.92s of the 2407.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r143_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 1509.7s of the 2407.74s of remaining time.\n",
      "\t-90.9869\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 1508.75s of the 2406.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r134_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 1508.6s of the 2406.64s of remaining time.\n",
      "\t-111.2706\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 1507.99s of the 2406.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r94_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 1507.77s of the 2405.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-94.1931\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.19s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 1452.34s of the 2350.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r128_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 1452.09s of the 2350.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r111_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 1451.94s of the 2349.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-91.9803\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.56s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 1432.18s of the 2330.22s of remaining time.\n",
      "\t-114.9748\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 1431.52s of the 2329.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r65_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 1431.37s of the 2329.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r88_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r30_BAG_L1 ... Training model for up to 1431.23s of the 2329.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r30_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: XGBoost_r49_BAG_L1 ... Training model for up to 1431.03s of the 2329.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r49_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: CatBoost_r5_BAG_L1 ... Training model for up to 1430.8s of the 2328.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r5_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 1430.59s of the 2328.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-90.7039\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.67s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 1408.72s of the 2306.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-94.3534\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.03s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r143_BAG_L1 ... Training model for up to 1399.51s of the 2297.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r143_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 1399.28s of the 2297.32s of remaining time.\n",
      "\t-97.8001\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166_BAG_L1 ... Training model for up to 1398.52s of the 2296.56s of remaining time.\n",
      "\t-119.7447\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31_BAG_L1 ... Training model for up to 1397.68s of the 2295.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r31_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r185_BAG_L1 ... Training model for up to 1397.45s of the 2295.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-95.6679\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.82s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r160_BAG_L1 ... Training model for up to 1371.41s of the 2269.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r160_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r60_BAG_L1 ... Training model for up to 1371.23s of the 2269.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r60_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: RandomForest_r15_BAG_L1 ... Training model for up to 1371.02s of the 2269.06s of remaining time.\n",
      "\t-89.2384\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.72s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135_BAG_L1 ... Training model for up to 1370.11s of the 2268.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r135_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: XGBoost_r22_BAG_L1 ... Training model for up to 1369.89s of the 2267.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r22_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r69_BAG_L1 ... Training model for up to 1369.67s of the 2267.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r69_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r6_BAG_L1 ... Training model for up to 1369.52s of the 2267.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r6_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r138_BAG_L1 ... Training model for up to 1369.34s of the 2267.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r138_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r121_BAG_L1 ... Training model for up to 1369.18s of the 2267.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r121_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r172_BAG_L1 ... Training model for up to 1368.98s of the 2267.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r172_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r180_BAG_L1 ... Training model for up to 1368.83s of the 2266.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r180_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r76_BAG_L1 ... Training model for up to 1368.64s of the 2266.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-92.8741\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.62s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 1359.83s of the 2257.87s of remaining time.\n",
      "\t-95.5664\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121_BAG_L1 ... Training model for up to 1358.41s of the 2256.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-92.9535\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.96s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r127_BAG_L1 ... Training model for up to 1316.26s of the 2214.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r127_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: RandomForest_r16_BAG_L1 ... Training model for up to 1316.07s of the 2214.11s of remaining time.\n",
      "\t-90.1295\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194_BAG_L1 ... Training model for up to 1314.26s of the 2212.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r194_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r12_BAG_L1 ... Training model for up to 1314.1s of the 2212.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r12_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r135_BAG_L1 ... Training model for up to 1313.89s of the 2211.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-93.2066\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.42s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r4_BAG_L1 ... Training model for up to 1277.23s of the 2175.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r4_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 1277.06s of the 2175.1s of remaining time.\n",
      "\t-122.9298\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r36_BAG_L1 ... Training model for up to 1276.35s of the 2174.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-89.7894\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.36s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r100_BAG_L1 ... Training model for up to 1252.79s of the 2150.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r100_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r163_BAG_L1 ... Training model for up to 1252.62s of the 2150.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r163_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: CatBoost_r198_BAG_L1 ... Training model for up to 1252.4s of the 2150.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r198_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r187_BAG_L1 ... Training model for up to 1252.2s of the 2150.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r187_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r19_BAG_L1 ... Training model for up to 1252.04s of the 2150.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-94.3399\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.51s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: XGBoost_r95_BAG_L1 ... Training model for up to 1241.33s of the 2139.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r95_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: XGBoost_r34_BAG_L1 ... Training model for up to 1241.08s of the 2139.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r34_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: LightGBM_r42_BAG_L1 ... Training model for up to 1240.85s of the 2138.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r42_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r1_BAG_L1 ... Training model for up to 1240.64s of the 2138.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-95.3757\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.96s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r89_BAG_L1 ... Training model for up to 1213.47s of the 2111.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-93.5008\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.32s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2084.82s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForest_r15_BAG_L1': 0.227, 'NeuralNetTorch_r87_BAG_L1': 0.16, 'RandomForestMSE_BAG_L1': 0.133, 'NeuralNetTorch_r22_BAG_L1': 0.133, 'NeuralNetTorch_r36_BAG_L1': 0.133, 'NeuralNetTorch_r31_BAG_L1': 0.12, 'NeuralNetTorch_r30_BAG_L1': 0.053, 'KNeighborsDist_BAG_L1': 0.027, 'RandomForest_r195_BAG_L1': 0.013}\n",
      "\t-85.7679\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2084.53s of the 2084.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2084.3s of the 2084.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 2084.1s of the 2083.92s of remaining time.\n",
      "\t-84.5727\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 2081.49s of the 2081.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 2081.28s of the 2081.1s of remaining time.\n",
      "\t-85.1166\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2079.69s of the 2079.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2079.52s of the 2079.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2079.3s of the 2079.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.5473\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.55s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2064.54s of the 2064.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 2064.31s of the 2064.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 2064.1s of the 2063.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.8671\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.01s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 2043.84s of the 2043.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 2043.59s of the 2043.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 2043.42s of the 2043.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 2043.21s of the 2043.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 2043.01s of the 2042.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-85.9697\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.07s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 2016.71s of the 2016.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 2016.45s of the 2016.27s of remaining time.\n",
      "\t-85.5881\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 2015.13s of the 2014.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 2014.89s of the 2014.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 2014.71s of the 2014.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 2014.51s of the 2014.33s of remaining time.\n",
      "\t-84.4596\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 2012.44s of the 2012.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 2012.22s of the 2012.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 2012.05s of the 2011.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 2011.82s of the 2011.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.376\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.06s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 1982.51s of the 1982.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r130_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 1982.26s of the 1982.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.3075\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.29s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L2 ... Training model for up to 1966.76s of the 1966.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r50_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L2 ... Training model for up to 1966.51s of the 1966.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r11_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_r194_BAG_L2 ... Training model for up to 1966.34s of the 1966.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r194_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: ExtraTrees_r172_BAG_L2 ... Training model for up to 1966.12s of the 1965.94s of remaining time.\n",
      "\t-84.6749\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L2 ... Training model for up to 1965.28s of the 1965.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r69_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L2 ... Training model for up to 1965.07s of the 1964.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r103_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r14_BAG_L2 ... Training model for up to 1964.89s of the 1964.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.6015\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.14s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L2 ... Training model for up to 1957.53s of the 1957.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r161_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L2 ... Training model for up to 1957.3s of the 1957.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r143_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r70_BAG_L2 ... Training model for up to 1957.11s of the 1956.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r70_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L2 ... Training model for up to 1956.91s of the 1956.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r156_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r196_BAG_L2 ... Training model for up to 1956.74s of the 1956.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r196_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r39_BAG_L2 ... Training model for up to 1956.53s of the 1956.35s of remaining time.\n",
      "\t-84.4661\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L2 ... Training model for up to 1954.89s of the 1954.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r167_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L2 ... Training model for up to 1954.69s of the 1954.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r95_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r41_BAG_L2 ... Training model for up to 1954.5s of the 1954.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.2744\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.22s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L2 ... Training model for up to 1942.07s of the 1941.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r98_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: LightGBM_r15_BAG_L2 ... Training model for up to 1941.86s of the 1941.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r15_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L2 ... Training model for up to 1941.63s of the 1941.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.6812\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.68s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r86_BAG_L2 ... Training model for up to 1913.71s of the 1913.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r86_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L2 ... Training model for up to 1913.46s of the 1913.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r37_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r197_BAG_L2 ... Training model for up to 1913.28s of the 1913.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.1397\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.27s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r49_BAG_L2 ... Training model for up to 1904.79s of the 1904.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r49_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTrees_r49_BAG_L2 ... Training model for up to 1904.55s of the 1904.37s of remaining time.\n",
      "\t-86.727\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L2 ... Training model for up to 1903.8s of the 1903.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r143_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForest_r127_BAG_L2 ... Training model for up to 1903.57s of the 1903.39s of remaining time.\n",
      "\t-84.0276\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L2 ... Training model for up to 1902.06s of the 1901.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r134_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: RandomForest_r34_BAG_L2 ... Training model for up to 1901.88s of the 1901.7s of remaining time.\n",
      "\t-86.0727\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L2 ... Training model for up to 1901.15s of the 1900.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r94_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L2 ... Training model for up to 1900.92s of the 1900.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.044\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.98s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_r128_BAG_L2 ... Training model for up to 1871.68s of the 1871.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r128_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L2 ... Training model for up to 1871.43s of the 1871.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r111_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r31_BAG_L2 ... Training model for up to 1871.25s of the 1871.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.7274\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.83s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r4_BAG_L2 ... Training model for up to 1858.19s of the 1858.01s of remaining time.\n",
      "\t-85.8814\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L2 ... Training model for up to 1857.5s of the 1857.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r65_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L2 ... Training model for up to 1857.32s of the 1857.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r88_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r30_BAG_L2 ... Training model for up to 1857.16s of the 1856.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r30_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: XGBoost_r49_BAG_L2 ... Training model for up to 1856.94s of the 1856.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r49_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: CatBoost_r5_BAG_L2 ... Training model for up to 1856.72s of the 1856.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r5_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r87_BAG_L2 ... Training model for up to 1856.5s of the 1856.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.4147\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.86s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r71_BAG_L2 ... Training model for up to 1842.42s of the 1842.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.6385\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.11s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r143_BAG_L2 ... Training model for up to 1836.1s of the 1835.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r143_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTrees_r178_BAG_L2 ... Training model for up to 1835.85s of the 1835.67s of remaining time.\n",
      "\t-85.3001\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166_BAG_L2 ... Training model for up to 1835.04s of the 1834.87s of remaining time.\n",
      "\t-86.0611\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31_BAG_L2 ... Training model for up to 1834.21s of the 1834.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r31_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r185_BAG_L2 ... Training model for up to 1833.98s of the 1833.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-89.0118\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.42s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r160_BAG_L2 ... Training model for up to 1814.32s of the 1814.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r160_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r60_BAG_L2 ... Training model for up to 1814.13s of the 1813.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r60_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: RandomForest_r15_BAG_L2 ... Training model for up to 1813.89s of the 1813.71s of remaining time.\n",
      "\t-84.2958\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135_BAG_L2 ... Training model for up to 1812.49s of the 1812.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r135_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: XGBoost_r22_BAG_L2 ... Training model for up to 1812.28s of the 1812.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r22_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r69_BAG_L2 ... Training model for up to 1812.05s of the 1811.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r69_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r6_BAG_L2 ... Training model for up to 1811.86s of the 1811.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r6_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r138_BAG_L2 ... Training model for up to 1811.64s of the 1811.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r138_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: LightGBM_r121_BAG_L2 ... Training model for up to 1811.47s of the 1811.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r121_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r172_BAG_L2 ... Training model for up to 1811.25s of the 1811.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r172_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r180_BAG_L2 ... Training model for up to 1811.08s of the 1810.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r180_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r76_BAG_L2 ... Training model for up to 1810.86s of the 1810.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.7059\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.11s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r197_BAG_L2 ... Training model for up to 1802.55s of the 1802.37s of remaining time.\n",
      "\t-85.1166\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121_BAG_L2 ... Training model for up to 1800.93s of the 1800.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.62\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.32s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r127_BAG_L2 ... Training model for up to 1777.38s of the 1777.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r127_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: RandomForest_r16_BAG_L2 ... Training model for up to 1777.17s of the 1776.99s of remaining time.\n",
      "\t-84.5727\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194_BAG_L2 ... Training model for up to 1774.47s of the 1774.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r194_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r12_BAG_L2 ... Training model for up to 1774.29s of the 1774.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r12_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r135_BAG_L2 ... Training model for up to 1774.05s of the 1773.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-88.1987\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.29s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r4_BAG_L2 ... Training model for up to 1747.51s of the 1747.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r4_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: ExtraTrees_r126_BAG_L2 ... Training model for up to 1747.3s of the 1747.12s of remaining time.\n",
      "\t-86.9694\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r36_BAG_L2 ... Training model for up to 1746.57s of the 1746.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.4847\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.26s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r100_BAG_L2 ... Training model for up to 1734.09s of the 1733.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r100_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r163_BAG_L2 ... Training model for up to 1733.88s of the 1733.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r163_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: CatBoost_r198_BAG_L2 ... Training model for up to 1733.65s of the 1733.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r198_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetFastAI_r187_BAG_L2 ... Training model for up to 1733.44s of the 1733.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r187_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: NeuralNetTorch_r19_BAG_L2 ... Training model for up to 1733.27s of the 1733.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-87.9062\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.32s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: XGBoost_r95_BAG_L2 ... Training model for up to 1725.75s of the 1725.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r95_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: XGBoost_r34_BAG_L2 ... Training model for up to 1725.49s of the 1725.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_r34_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: LightGBM_r42_BAG_L2 ... Training model for up to 1725.23s of the 1725.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r42_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r1_BAG_L2 ... Training model for up to 1725.02s of the 1724.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.9604\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.64s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r89_BAG_L2 ... Training model for up to 1703.15s of the 1702.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-86.7791\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.67s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1682.87s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForest_r127_BAG_L2': 0.516, 'NeuralNetTorch_r197_BAG_L2': 0.264, 'NeuralNetTorch_r87_BAG_L2': 0.143, 'ExtraTreesMSE_BAG_L2': 0.044, 'NeuralNetTorch_r22_BAG_L2': 0.022, 'KNeighborsDist_BAG_L1': 0.011}\n",
      "\t-82.9917\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1015.43s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240606_152445\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['석식계'] | 10 available columns: ['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:338\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_in:\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['석식계'] not in index\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m predictor_lunch \u001b[38;5;241m=\u001b[39m TabularPredictor(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m중식계\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(train_data\u001b[38;5;241m=\u001b[39mtrain_data, presets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_quality\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 테스트 데이터에서 예측 수행\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m predictions_lunch \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor_lunch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 예측 결과를 확인\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions_lunch)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1931\u001b[0m, in \u001b[0;36mTabularPredictor.predict\u001b[1;34m(self, data, model, as_pandas, transform_features, decision_threshold)\u001b[0m\n\u001b[0;32m   1929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decision_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1930\u001b[0m     decision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_threshold\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_pandas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecision_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:208\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict\u001b[1;34m(self, X, model, as_pandas, inverse_transform, transform_features, decision_threshold)\u001b[0m\n\u001b[0;32m    206\u001b[0m     decision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    207\u001b[0m X_index \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(X\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_multiclass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_features\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_cleaner\u001b[38;5;241m.\u001b[39mproblem_type_transform \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type\n\u001b[0;32m    212\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m get_pred_from_proba(y_pred_proba\u001b[38;5;241m=\u001b[39my_pred_proba, problem_type\u001b[38;5;241m=\u001b[39mproblem_type, decision_threshold\u001b[38;5;241m=\u001b[39mdecision_threshold)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:188\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict_proba\u001b[1;34m(self, X, model, as_pandas, as_multiclass, inverse_transform, transform_features)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform_features:\n\u001b[1;32m--> 188\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_trainer()\u001b[38;5;241m.\u001b[39mpredict_proba(X, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    190\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_predict_proba(\n\u001b[0;32m    191\u001b[0m     y_pred_proba\u001b[38;5;241m=\u001b[39my_pred_proba, as_pandas\u001b[38;5;241m=\u001b[39mas_pandas, index\u001b[38;5;241m=\u001b[39mX_index, as_multiclass\u001b[38;5;241m=\u001b[39mas_multiclass, inverse_transform\u001b[38;5;241m=\u001b[39minverse_transform\n\u001b[0;32m    192\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:464\u001b[0m, in \u001b[0;36mAbstractTabularLearner.transform_features\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_generators:\n\u001b[1;32m--> 464\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:344\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    343\u001b[0m             missing_cols\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m available columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator:\n\u001b[0;32m    350\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['석식계'] | 10 available columns: ['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴']\""
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기 (예제로 파일 경로는 'train.csv'와 'test.csv'를 사용)\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# 학습할 피처를 설정합니다. 여기서는 '중식계'와 '석식계'를 제외한 모든 컬럼을 사용합니다.\n",
    "features = ['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수',\n",
    "            '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴']\n",
    "\n",
    "\n",
    "# GPU 사용 설정\n",
    "#ag_args_fit = {'num_gpus': 1}\n",
    "\n",
    "# '중식계'를 예측하는 모델 학습\n",
    "#predictor_lunch = TabularPredictor(label='중식계').fit(train_data=train_data, presets='best_quality', ag_args_fit=ag_args_fit)\n",
    "\n",
    "# '중식계'를 예측하는 모델 학습\n",
    "predictor_lunch = TabularPredictor(label='중식계').fit(train_data=train_data, presets='best_quality')\n",
    "\n",
    "# 테스트 데이터에서 예측 수행\n",
    "predictions_lunch = predictor_lunch.predict(test_data[features])\n",
    "\n",
    "# 예측 결과를 확인\n",
    "print(predictions_lunch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830ae5f5-fc46-47b4-939c-54960c0a4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240607_080227\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 3600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240607_080227/ds_sub_fit/sub_fit_ho.\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240607_080227/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       7.71 GB / 31.92 GB (24.2%)\n",
      "Disk Space Avail:   189.60 GB / 475.62 GB (39.9%)\n",
      "===================================================\n",
      "Train Data Rows:    1071\n",
      "Train Data Columns: 11\n",
      "Label Column:       중식계\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    7898.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.83 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 316\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 2 | ['현본사소속재택근무자수', '석식계']\n",
      "\t\t('int', [])                        : 4 | ['본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수']\n",
      "\t\t('object', [])                     : 1 | ['요일']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['일자']\n",
      "\t\t('object', ['text'])               : 3 | ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   1 | ['요일']\n",
      "\t\t('category', ['text_as_category'])  :   3 | ['조식메뉴', '중식메뉴', '석식메뉴']\n",
      "\t\t('float', [])                       :   2 | ['현본사소속재택근무자수', '석식계']\n",
      "\t\t('int', [])                         :   4 | ['본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수']\n",
      "\t\t('int', ['binned', 'text_special']) :  43 | ['조식메뉴.char_count', '조식메뉴.word_count', '조식메뉴.capital_ratio', '조식메뉴.lower_ratio', '조식메뉴.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :   5 | ['일자', '일자.year', '일자.month', '일자.day', '일자.dayofweek']\n",
      "\t\t('int', ['text_ngram'])             : 249 | ['__nlp__.new', '__nlp__.가지나물', '__nlp__.가지나물 포기김치', '__nlp__.가지나물 포기김치 김치', '__nlp__.가쯔오장국', ...]\n",
      "\t4.0s = Fit runtime\n",
      "\t11 features in original data used to generate 307 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.99s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 597.19s of the 896.0s of remaining time.\n",
      "\t-246.7733\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 597.08s of the 895.89s of remaining time.\n",
      "\t-218.2133\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 596.95s of the 895.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 596.69s of the 895.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 596.44s of the 895.25s of remaining time.\n",
      "\t-89.9767\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 594.69s of the 893.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 594.44s of the 893.25s of remaining time.\n",
      "\t-96.3549\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 593.25s of the 892.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 593.05s of the 891.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 592.8s of the 891.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-96.507\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.34s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 544.28s of the 843.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 543.98s of the 842.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 543.73s of the 842.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-89.8816\t = Validation score   (-root_mean_squared_error)\n",
      "\t70.2s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 473.29s of the 772.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 473.04s of the 771.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 472.84s of the 771.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 472.6s of the 771.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.0.0`.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 472.37s of the 771.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m일자\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m요일\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m본사정원수\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m본사휴가자수\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m본사출장자수\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m본사시간외근무명령서승인건수\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m현본사소속재택근무자수\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m조식메뉴\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m중식메뉴\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m석식메뉴\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# '중식계'를 예측하는 모델 학습\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m predictor_lunch \u001b[38;5;241m=\u001b[39m \u001b[43mTabularPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m중식계\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_quality\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# '석식계'를 예측하는 모델 학습\u001b[39;00m\n\u001b[0;32m     16\u001b[0m predictor_dinner \u001b[38;5;241m=\u001b[39m TabularPredictor(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m석식계\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(train_data\u001b[38;5;241m=\u001b[39mtrain_data, presets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_quality\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1099\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_stacking:\n\u001b[0;32m   1094\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamic stacking is enabled (dynamic_stacking=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynamic_stacking\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[1;32m-> 1099\u001b[0m     num_stack_levels, time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_stacking\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mds_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (time_limit \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough time left to train models for the full fit. Consider specifying a larger time_limit. Time remaining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_limit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1186\u001b[0m, in \u001b[0;36mTabularPredictor._dynamic_stacking\u001b[1;34m(self, ag_fit_kwargs, ag_post_fit_kwargs, validation_procedure, detection_time_frac, holdout_frac, n_folds, n_repeats, memory_safe_fits, clean_up_fits, holdout_data)\u001b[0m\n\u001b[0;32m   1181\u001b[0m         ds_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds_fit_context\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ds_fit_context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/sub_fit_custom_ho\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1182\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1183\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting holdout-based sub-fit for dynamic stacking with custom validation data. Context path is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_fit_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds_fit_context\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1184\u001b[0m         )\n\u001b[1;32m-> 1186\u001b[0m     stacked_overfitting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sub_fit_memory_save_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;66;03m# Holdout is false, use (repeated) cross-validation\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     is_stratified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;129;01min\u001b[39;00m [REGRESSION, QUANTILE, SOFTCLASS]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1336\u001b[0m, in \u001b[0;36mTabularPredictor._sub_fit_memory_save_wrapper\u001b[1;34m(self, train_data, time_limit, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     normal_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normal_fit:\n\u001b[1;32m-> 1336\u001b[0m     stacked_overfitting \u001b[38;5;241m=\u001b[39m \u001b[43m_sub_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stacked_overfitting\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:4897\u001b[0m, in \u001b[0;36m_sub_fit\u001b[1;34m(predictor, train_data, time_limit, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[0m\n\u001b[0;32m   4894\u001b[0m clean_up_fits \u001b[38;5;241m=\u001b[39m ds_fit_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_up_fits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4896\u001b[0m predictor\u001b[38;5;241m.\u001b[39m_learner\u001b[38;5;241m.\u001b[39mset_contexts(path_context\u001b[38;5;241m=\u001b[39mds_fit_context)\n\u001b[1;32m-> 4897\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predictor\u001b[38;5;241m.\u001b[39mmodel_names():\n\u001b[0;32m   4900\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to determine stacked overfitting. AutoGluon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms sub-fit did not successfully train any models!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1115\u001b[0m, in \u001b[0;36mTabularPredictor._fit\u001b[1;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_post_fit_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:128\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 128\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[0;32m    142\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:125\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m log_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, log_str)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2503\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_val)\n\u001b[0;32m   2502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m-> 2503\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2508\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2516\u001b[0m     \u001b[38;5;66;03m# TODO v1.0: Add toggle to raise exception if no models trained\u001b[39;00m\n\u001b[0;32m   2517\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: AutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:388\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    386\u001b[0m         core_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_core)\n\u001b[0;32m    387\u001b[0m         aux_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 388\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:536\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[0m\n\u001b[0;32m    534\u001b[0m     core_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m    535\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m--> 536\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m aux_models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_weighted_ensemble:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:666\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2453\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2452\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 2453\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2464\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2302\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2301\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2302\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2315\u001b[0m multi_fold_time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m multi_fold_time_start\n\u001b[0;32m   2316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2410\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2408\u001b[0m         time_start_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2409\u001b[0m         time_left \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time_start_model \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[1;32m-> 2410\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   2412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2183\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   2179\u001b[0m         bagged_model_fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[0;32m   2180\u001b[0m             k_fold\u001b[38;5;241m=\u001b[39mk_fold, k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start, k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end, n_repeats\u001b[38;5;241m=\u001b[39mn_repeats, n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start\n\u001b[0;32m   2181\u001b[0m         )\n\u001b[0;32m   2182\u001b[0m         model_fit_kwargs\u001b[38;5;241m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 2183\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   2197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1817\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1815\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1819\u001b[0m fit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1763\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, total_resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AbstractModel:\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;124;03m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1763\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:854\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 854\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py:165\u001b[0m, in \u001b[0;36mStackerEnsembleModel._fit\u001b[1;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     time_limit \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:273\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit\u001b[1;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;66;03m# Reserve time for final refit model\u001b[39;00m\n\u001b[0;32m    272\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m folds_to_fit \u001b[38;5;241m/\u001b[39m (folds_to_fit \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.2\u001b[39m)\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_folds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_bag_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# FIXME: Cleanup self\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refit_folds:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:689\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit_folds\u001b[1;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n\u001b[0;32m    688\u001b[0m     fold_fitting_strategy\u001b[38;5;241m.\u001b[39mschedule_fold_model_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfold_fit_args)\n\u001b[1;32m--> 689\u001b[0m \u001b[43mfold_fitting_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_all_folds_scheduled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# No need to add child times or save child here as this already occurred in the fold_fitting_strategy\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_child(model\u001b[38;5;241m=\u001b[39mmodel, add_child_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:309\u001b[0m, in \u001b[0;36mSequentialLocalFoldFittingStrategy.after_all_folds_scheduled\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_all_folds_scheduled\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs:\n\u001b[1;32m--> 309\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_fold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:314\u001b[0m, in \u001b[0;36mSequentialLocalFoldFittingStrategy._fit_fold_model\u001b[1;34m(self, fold_ctx)\u001b[0m\n\u001b[0;32m    312\u001b[0m time_start_fold \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    313\u001b[0m time_limit_fold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fold_time_limit(fold_ctx)\n\u001b[1;32m--> 314\u001b[0m fold_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_start_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_base_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m fold_model, pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_oof(fold_model, fold_ctx)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_bagged_ensemble(fold_model, pred_proba, fold_ctx)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:349\u001b[0m, in \u001b[0;36mSequentialLocalFoldFittingStrategy._fit\u001b[1;34m(self, model_base, time_start_fold, time_limit_fold, fold_ctx, kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m     num_cpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cpus, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_resources_per_job\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_cpus\u001b[39m\u001b[38;5;124m\"\u001b[39m, math\u001b[38;5;241m.\u001b[39minf))\n\u001b[0;32m    348\u001b[0m     num_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_gpus, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_resources_per_job\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m, math\u001b[38;5;241m.\u001b[39minf))\n\u001b[1;32m--> 349\u001b[0m \u001b[43mfold_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m fold_model\u001b[38;5;241m.\u001b[39mfit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_start_fold\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fold_model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:854\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 854\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:208\u001b[0m, in \u001b[0;36mTabularNeuralNetTorchModel._fit\u001b[1;34m(self, X, y, X_val, y_val, time_limit, sample_weight, num_cpus, num_gpus, reporter, verbosity, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeLimitExceeded\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# train network\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreporter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:311\u001b[0m, in \u001b[0;36mTabularNeuralNetTorchModel._train_net\u001b[1;34m(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, time_limit, reporter, verbosity)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 311\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    313\u001b[0m total_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기 (예제로 파일 경로는 'train.csv'와 'test.csv'를 사용)\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# 학습할 피처를 설정합니다. 여기서는 '중식계'와 '석식계'를 제외한 모든 컬럼을 사용합니다.\n",
    "features = ['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수',\n",
    "            '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴']\n",
    "\n",
    "# '중식계'를 예측하는 모델 학습\n",
    "predictor_lunch = TabularPredictor(label='중식계').fit(train_data=train_data, presets='best_quality')\n",
    "\n",
    "# '석식계'를 예측하는 모델 학습\n",
    "predictor_dinner = TabularPredictor(label='석식계').fit(train_data=train_data, presets='best_quality')\n",
    "\n",
    "# 테스트 데이터에서 예측 수행\n",
    "predictions_lunch = predictor_lunch.predict(test_data[features])\n",
    "predictions_dinner = predictor_dinner.predict(test_data[features])\n",
    "\n",
    "# 예측 결과를 확인\n",
    "print(predictions_lunch)\n",
    "print(predictions_dinner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cd0f70-c8dd-49b0-b4d4-d2b04046dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요하다면 예측 결과를 CSV 파일로 저장\n",
    "# test_data['중식계 예측'] = predictions_lunch\n",
    "# test_data['석식계 예측'] = predictions_dinner\n",
    "# test_data.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7edb90c1-e5de-4ded-8034-fa50780ac50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: train.csv | Columns = 12 / 12 | Rows = 1205 -> 1205\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['석식계'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m TabularDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 예시 파일 경로\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_l \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m석식계\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5209\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5346\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5350\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5351\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['석식계'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = TabularDataset('train.csv')  # 예시 파일 경로\n",
    "train_l = train_data.drop('석식계')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa94856-1ccc-4bbf-8db7-a6fcc814d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: train.csv | Columns = 12 / 12 | Rows = 1205 -> 1205\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240303_221004\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 목표 변수(레이블) 이름\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m predictor \u001b[38;5;241m=\u001b[39m TabularPredictor(label\u001b[38;5;241m=\u001b[39mlabel)\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mgargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1003\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     inferred_problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m     inferred_problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner\u001b[38;5;241m.\u001b[39minfer_problem_type(y\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel], silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1005\u001b[0m num_bag_folds, num_bag_sets, num_stack_levels, dynamic_stacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_stack_args(\n\u001b[0;32m   1006\u001b[0m     num_bag_folds\u001b[38;5;241m=\u001b[39mnum_bag_folds,\n\u001b[0;32m   1007\u001b[0m     num_bag_sets\u001b[38;5;241m=\u001b[39mnum_bag_sets,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     dynamic_stacking\u001b[38;5;241m=\u001b[39mdynamic_stacking,\n\u001b[0;32m   1014\u001b[0m )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_stack:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = TabularDataset('train.csv')  # 예시 파일 경로\n",
    "label = '중식계'  # 목표 변수(레이블) 이름\n",
    "\n",
    "# 모델 학습\n",
    "predictor = TabularPredictor(label=label).fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31de4a8-39f8-40de-96b1-cfc47e974638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: test.csv | Columns = 10 / 10 | Rows = 50 -> 50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['석식계'] | 10 available columns: ['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:338\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_in:\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m         X \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_in]\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['석식계'] not in index\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_df \u001b[38;5;241m=\u001b[39m TabularDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 예시 파일 경로\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(test_df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1931\u001b[0m, in \u001b[0;36mTabularPredictor.predict\u001b[1;34m(self, data, model, as_pandas, transform_features, decision_threshold)\u001b[0m\n\u001b[0;32m   1929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decision_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1930\u001b[0m     decision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_threshold\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m=\u001b[39mdata, model\u001b[38;5;241m=\u001b[39mmodel, as_pandas\u001b[38;5;241m=\u001b[39mas_pandas, transform_features\u001b[38;5;241m=\u001b[39mtransform_features, decision_threshold\u001b[38;5;241m=\u001b[39mdecision_threshold)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:208\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict\u001b[1;34m(self, X, model, as_pandas, inverse_transform, transform_features, decision_threshold)\u001b[0m\n\u001b[0;32m    206\u001b[0m     decision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    207\u001b[0m X_index \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(X\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(\n\u001b[0;32m    209\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, model\u001b[38;5;241m=\u001b[39mmodel, as_pandas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, as_multiclass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inverse_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform_features\u001b[38;5;241m=\u001b[39mtransform_features\n\u001b[0;32m    210\u001b[0m )\n\u001b[0;32m    211\u001b[0m problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_cleaner\u001b[38;5;241m.\u001b[39mproblem_type_transform \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type\n\u001b[0;32m    212\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m get_pred_from_proba(y_pred_proba\u001b[38;5;241m=\u001b[39my_pred_proba, problem_type\u001b[38;5;241m=\u001b[39mproblem_type, decision_threshold\u001b[38;5;241m=\u001b[39mdecision_threshold)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:188\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict_proba\u001b[1;34m(self, X, model, as_pandas, as_multiclass, inverse_transform, transform_features)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform_features:\n\u001b[1;32m--> 188\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_features(X)\n\u001b[0;32m    189\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_trainer()\u001b[38;5;241m.\u001b[39mpredict_proba(X, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    190\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_predict_proba(\n\u001b[0;32m    191\u001b[0m     y_pred_proba\u001b[38;5;241m=\u001b[39my_pred_proba, as_pandas\u001b[38;5;241m=\u001b[39mas_pandas, index\u001b[38;5;241m=\u001b[39mX_index, as_multiclass\u001b[38;5;241m=\u001b[39mas_multiclass, inverse_transform\u001b[38;5;241m=\u001b[39minverse_transform\n\u001b[0;32m    192\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:464\u001b[0m, in \u001b[0;36mAbstractTabularLearner.transform_features\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_generators:\n\u001b[1;32m--> 464\u001b[0m         X \u001b[38;5;241m=\u001b[39m feature_generator\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:344\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    343\u001b[0m             missing_cols\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m available columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator:\n\u001b[0;32m    350\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['석식계'] | 10 available columns: ['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴']\""
     ]
    }
   ],
   "source": [
    "test_df = TabularDataset('test.csv')  # 예시 파일 경로\n",
    "predictions = predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e914224-01f6-4ff2-a106-5576bd4ff000",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = predictor.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
